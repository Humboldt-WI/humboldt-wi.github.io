<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Crime and Neural Nets</title>
  <meta name="author" content="" />

  
  <meta name="keywords" content="devows, hugo, go">	
  

  
  <meta name="description" content="Introducing Recurrent Neural Networks with Long-Short-Term Memory and Gated Recurrent Unit to predict reported Crime Incident">
  

  <meta name="generator" content="Hugo 0.54.0" />

  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="https://humboldt-wi.github.io/blog/css/animate.css" rel="stylesheet">

  
  
    <link href="https://humboldt-wi.github.io/blog/css/style.blue.css" rel="stylesheet" id="theme-stylesheet">
  


  
  <link href="https://humboldt-wi.github.io/blog/css/custom.css" rel="stylesheet">

  
  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
  <link rel="shortcut icon" href="https://humboldt-wi.github.io/blog/img/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="https://humboldt-wi.github.io/blog/img/apple-touch-icon.png" />
  

  <link href="https://humboldt-wi.github.io/blog/css/owl.carousel.css" rel="stylesheet">
  <link href="https://humboldt-wi.github.io/blog/css/owl.theme.css" rel="stylesheet">

  <link rel="alternate" href="https://humboldt-wi.github.io/index.xml" type="application/rss+xml" title="Institute of Infomation Systems at HU-Berlin">

  
  <meta property="og:title" content="Crime and Neural Nets" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="/blog/research/information_systems_1819/02lstmgruandbeyond//" />
  <meta property="og:image" content="img/logoGross.png" />

</head>


  <body>

    <div id="all">

        <header>

          <div class="navbar-affixed-top" data-spy="affix" data-offset-top="200">

    <div class="navbar navbar-default yamm" role="navigation" id="navbar">

        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="https://humboldt-wi.github.io/blog/">
                    <img src="https://humboldt-wi.github.io/blog/img/logo.png" alt="Crime and Neural Nets logo" class="hidden-xs hidden-sm">
                    <img src="https://humboldt-wi.github.io/blog/img/logo-small.png" alt="Crime and Neural Nets logo" class="visible-xs visible-sm">
                    <span class="sr-only">Crime and Neural Nets - go to homepage</span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only">Toggle Navigation</span>
                        <i class="fa fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  <li class="dropdown">
                    
                    <a href="/blog/">Home</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/news/">News</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/contributors/">Contributors</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/research/">research</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/contact/">Contact</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">

                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">

                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>

                </span>
                    </div>
                </form>

            </div>
            

        </div>
    </div>
    

</div>




        </header>

        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Crime and Neural Nets</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">

                <div class="row">

                    

                    <div class="col-md-9" id="blog-post">

                        <p class="text-muted text-uppercase mb-small text-right">By <a href="#">Seminar Information Systems (WS18/19)</a> | February 7, 2019</p>

                        <div id="post-content">
                          
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Crime-and-Neural-Nets">Crime and Neural Nets<a class="anchor-link" href="#Crime-and-Neural-Nets">&#182;</a></h1><h2 id="Introducing-Recurrent-Neural-Networks-with-Long-Short-Term-Memory-and-Gated-Recurrent-Unit-to-predict-reported-Crime-Incidents">Introducing Recurrent Neural Networks with Long-Short-Term Memory and Gated Recurrent Unit to predict reported Crime Incidents<a class="anchor-link" href="#Introducing-Recurrent-Neural-Networks-with-Long-Short-Term-Memory-and-Gated-Recurrent-Unit-to-predict-reported-Crime-Incidents">&#182;</a></h2><h4 id="Carolin-Kunze,-Marc-Scheu,-Thomas-Siskos"><em>Carolin Kunze, Marc Scheu, Thomas Siskos</em><a class="anchor-link" href="#Carolin-Kunze,-Marc-Scheu,-Thomas-Siskos">&#182;</a></h4><p><img align="right" src="/blog/img/seminar/lstm_gru_1819/crime_intro.png" alt="crime intro" style="width: 500px;"/>
Several police departments across the Unites States have been experimenting with software for crime prdiction. This  started a controversial debate: Critics are questioning the predictiv power of the underlying machine learning models and point out biases towards certain crime typs and neighborhoods. We took this as occacion to look into the publicly available <a href="https://catalog.data.gov/dataset/crimes-2001-to-present-398a4">crime records of the city of chicago</a>. The data set contains close to 7 milliom reported crime incidences starting from 2001 until the precceding week of the data download. It includes 22 columns with detailed information, such as type of the offence,  geo location,  district and the time of crime occurence. We will focus on temporal patterns in the data by predicting crime frequencies per day on the district level. Such a model may be used in practice to faciliate personel planning across the districts for the chigaco police force. We generate the input data for model building by counting the reported crimes by day and district. This results in a table with three columns containing date, district and the respective counts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/crimes_district.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[1]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>District</th>
      <th>Date</th>
      <th>Incidents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>2001-01-01</td>
      <td>37.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>2001-01-02</td>
      <td>44.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>2001-01-03</td>
      <td>44.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>2001-01-04</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>2001-01-05</td>
      <td>67.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By changing this aggregation on the time or the locational level a similar model for more granular predictions might be build. This is under the assumption of enough representative data on a more detailed aggregation level. We will take the described use case as an example to introduce Recurrent Neural Networks (RNNs) with Long-Short-Term memory (LSTM) and Gated Recurrent Unit (GRU). First we explain the general concept behind RNNs and train a simple network using different input formats. We then introduce RNNs with LSTM and GRU to overcome the vanishing gradient problem. After explaining the forward pass and the derivation of LSTM and GRU we tune the hyperparameters for the three introduced RNN variants and compare their performance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Introduction-to-Recurrent-Neural-Networks">Introduction to Recurrent Neural Networks<a class="anchor-link" href="#Introduction-to-Recurrent-Neural-Networks">&#182;</a></h3><p>Many machine learning tasks are based on the assumption of identically independent distributed (i.i.d.) samples. Thus a prediction for a label $y_i$ only depends on the corresponding feature vector $\vec{x_i}$, where $i$ denotes the index of the sample. When working with sequential data, one typically wants to predict a future outcome $\vec{y_{t+1}}$, which often equals the following observable vector $\vec{x_{t+1}}$, from a sequence of previously observed vectors $\vec{x_t}$, …, $\vec{x_{t-T}}$. $t$ denotes the last observed point of time and $T$  the number of observations since the start of the sequence.
While the vectors within a sequence are assumed to be dependent, we might still work with multiple mutually independent time series in a dataset. In this case one could use de notation $\vec{x_{ti}}$, corresponding to the observed vector at time $t$ from the sample sequence $i$. For simplicity we omit the sample index below. Note that many sequential prediction tasks are based on one dimensional time series, in that case one may also omit the vector notation. However, we stick to the more general form. <br>
A simple feedforward network may be used to make predictions under the i.i.d assumption by passing one feature vector $\vec{x_i}$ at a time through the network. However, if one wants to account for temporal dependencies between the observed vectors $\vec{x_t}$, …, $\vec{x_{t-T}}$ the network needs some kind of memory. That’s the concept behind recurrent neural networks: They use hidden activations, referred to as hidden state, to carry information within the network from one observation to the following one.</p>
<p style='text-align: center;'><img align="center" src="/blog/img/seminar/lstm_gru_1819/RNN_t2.png" alt="RNN lag 2" style="width: 400px;"/><br>
Image 1: Recurrent Neural Network with a sequence length of two.</p><p>Image 1 illustrates how a simple RNN may be used to predict $\vec{y_{t+1}} (:= \vec{x_{t+1})} $ given a sequence of two observed vectors $\vec{x_t}$ and $\vec{x_{t-1}}$. The rectangles represent the input vectors. Each circle represents a vector of hidden activations and the triangle represents the output i.e. the predicted label. The arrows illustrate layer operations, i.e. the multiplication of the previous vector with one of the weight matrices $W_{in}$, $W_h$, or $W_{out}$, denoted by different colours. To make a prediction for $\vec{y_{t+1}}$, the first vector in the sequence $\vec{x_{t-1}}$ gets multiplied with the input weights $W_{in}$ and the activation function is applied to produce the first hidden state $\vec{h_{t-1}}$. Since $\vec{x_{t-1}}$ is the first observation in the sequence, $\vec{h_{t-1}}$ represents all available information at the time step $t-1$. To carry this information to the next hidden state $\vec{h_t}$, $\vec{h_{t-1}}$ gets multiplied with the hidden weight matrix $W_h$. Consecutively the new input $\vec{x_t}$ is multiplied with the input matrix $W_{in}$. The resulting vector carries the new information extracted from $\vec{x_t}$. This newly available information needs to be combined with the knowledge about previous observations to create $\vec{h_t}$. This can be done by summing up the two outputs from the matrix products and the bias $b_h$ and applying the  hyperbolic tangents as activation function to the vector sum (Equation 1). This is illustrated by the intersect of the green and the orange arrow. Conceptually the weights in $W_h$ represent the importance of previous observations and the weights in $W_{in}$ represent the importance of new information for the prediction task.
All available information at the time step $t$ is now aggregated in $\vec{h_t}$. To make a prediction for $\vec{y_{t+1}}$ it gets multiplied with the output weight matrix $W_{out}$ (Eqation 2). The size of $W_{out}$ and the corresponding activation function are, as usually, defined by the prediction task (regression or classification). This steps cover the forward pass in a simple RNN.
<br>
<br>
$\vec{h_t} = tanh(W_{in} * \vec{h_{t-1}} +  \vec{x_{t}} +\vec{b_{h}} )$  (Eqation 1)
<br>
$\vec{y_{t+1}} = ReLu(W_{out} * \vec{h_t} + \vec{b_{out}} )$ (Eqation 2)
<br>
<br>
If we want to look back more time steps when making a prediction for $\vec{y_{t+1}}$, we just need to add another hidden state and another input for every additional time step. Image 3 illustrates how this may look for a sequence of length three. Interestingly, even though RNNs can get quite deep along the time dimension, they have relatively few parameters. This is the case because all inputs or hidden states represent the same thing, another time step in the series or another aggregation of past observations. Thus, we only have to train three different weight matrices, one for the input, one for the hidden state and one for the output.<br></p>
<p style='text-align: center;'><img  src="/blog/img/seminar/lstm_gru_1819/RNN_t3.png" alt="RNN lag 3" style="width: 500px;"/><br>
Image 2: Recurrent neural network with a sequence length of three.</p><p>Drawing RNNs with more time steps becomes cumbersome. We have to keep adding inputs and hidden states. The same holds true if we would like to construct them in in python. A more elegant and flexible representation is given in Image 4. Compared to Image 3 the repetitive elements have been put into a loop. For every time step in the sequence length $T$, the new input is feed into the network and combined with the previous hidden state. The new hidden state is passed back to the start of the loop as input to the next iteration. The loop stops at the end of the sequence and the last hidden state is put through the output layer. To include the first timestep within the loop representation, an initial hidden state $\vec{h_0}$ is defined as a vector of zeros.</p>
<p style='text-align: center;'><img  src="/blog/img/seminar/lstm_gru_1819/RNN_Loop.png" alt="NN" style="width: 350px;"/><br>
Image 3: Recurrent neural network with a loop and a sequence length of $T$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Keras provides a predefined model layer that constructs the described RNN behind the scenes. However, to emphasise the described operations, we demonstrate how to construct it by yourself using fully connected layers and the keras functional API. In keras a layer refers to a set of layer operations, typically a matrix multiplication and the application of an activation function, that output a new vector of activations. We follow this notation below.<br>
The functional  API allows to define a list of inputs and put them through several layers to produce a (list of) output(s). It then connects inputs and output(s) so the keras backend can be used to derive the gradients and train the model. We start implementing the RNN by initializing the <code>input_list</code>. The basic properties of a simple RNN are defined by the number of the hidden activations <code>H_SZ</code> , the sequence length <code>SEQ_LEN</code> and the number of features <code>FEAT_SZ</code> per time step. By choosing <code>H_SZ</code>, the shape of the three weight matrices is sufficiently defined: The second and first dimension of $W_{in}$ and $W_{out}$ are respectively defined by the shape of the input data and the labels. Moreover, the first dimension of $W_{in}$ must match the first dimension of $W_h$, so their outputs can be added together. $W_h$ must be squared because the result of the matrix product $W_h$ x $h_t$ gets again multiplied with $W_h$. Since $h_t$ gets also multiplied by $W_{out}$ the second dimension of $W_{out}$ must also be <code>H_SZ</code>. We set <code>H_SZ</code> to five and <code>SEQ_LEN</code> to ten after little manual tuning. In general, the parameters did not seem to have a great impact on the model performance. Since the crime data consists of one-dimensional time series <code>FEAT_SZ</code> is given as one.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="k">import</span> <span class="n">metrics</span>

<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">SimpleRNN</span><span class="p">,</span> <span class="n">TimeDistributed</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>


<span class="c1"># create list to keep track of inputs</span>
<span class="n">input_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># initialize RNN properties</span>
<span class="n">H_SZ</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">FEAT_SZ</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>Dense()</code> layer in keras holds the weights of a fully connected layer. It is initialized by passing the output dimension to the <code>Dense()</code> class. The input dimension will be interfered from the first input. After initialization, it can be called on a tensor which will return the resulting output activations. The three layers of the network are initialized by passing the respective number of output activations <code>H_SZ</code>, <code>H_SZ</code> and <code>FEAT_SZ</code> to the <code>Dense()</code> class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># initilize fully connected layers and the activation function of the hidden state</span>
<span class="n">first_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">H_SZ</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;first_layer&#39;</span><span class="p">)</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">H_SZ</span> <span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;hidden_layer&#39;</span><span class="p">)</span>
<span class="n">tanh</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;tanh&#39;</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">FEAT_SZ</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;output_layer&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We  loop through the sequence length and create for every time step a new input <code>x</code> and append it to the input list. The input activations <code>act_in</code> are calculated for every time step by putting the input variable <code>x</code> through the first layer. We additionally initialize <code>h0</code> as a vector of zeros with the same (variable) shape as the input activations at the first time step. The new hidden state <code>ht</code> gets updated implementing Equation 1. After the loop the final hidden state is put through the output layer to make a prediction. Finally the functional API <code>Model()</code> connects input and output variables to create <code>myRNN</code>. Since keras typically works with tensors a function that splits a three-dimensional tensor along the second dimension, which corresponds to time dimension in keras, is defined.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">SEQ_LEN</span><span class="p">):</span>

  <span class="c1"># get input vector and append it to input list</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">FEAT_SZ</span><span class="p">,))</span>
  <span class="n">input_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="c1"># calculate input actiavtions</span>
  <span class="n">act_in</span> <span class="o">=</span> <span class="n">first_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
  <span class="c1"># initialize h0 with zeros and append it to input list</span>
    <span class="n">h0</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">act_in</span><span class="p">)</span>
    <span class="n">input_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h0</span><span class="p">)</span>
    <span class="n">ht</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">h0</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;h0&#39;</span><span class="p">)</span>

  <span class="c1"># calculate hidden activations</span>
  <span class="n">ht</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">add</span><span class="p">([</span><span class="n">hidden_layer</span><span class="p">(</span><span class="n">ht</span><span class="p">),</span> <span class="n">act_in</span><span class="p">]))</span>

<span class="c1"># calculate output</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">output_layer</span><span class="p">(</span><span class="n">ht</span><span class="p">)</span>

<span class="n">myRNN</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_list</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_input</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
  <span class="c1"># slice 3-dim tensor along 2nd axis into list of inputs</span>
  <span class="k">return</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Formatting-sequential-data">Formatting sequential data<a class="anchor-link" href="#Formatting-sequential-data">&#182;</a></h3><p>We load the reported crimes, counte by district and day and change the long-format in wide-format to get a matrix of 24 districts by 6513 timsteps.
By training one model on the time series from all districts, we assume each series to be a different realization of the same underlying sequential pattern. An obvious alternative would be to train a separate model for each district.
Before further formatting we split the data into a train and a validation set. The validation set should resemble the data, the model would see in production. We therefore use the data from 2015 - 2017 as validation set. We will later use the most resent crime counts from 2017 as test set.
One can not simply train a neural network on 24 long time series.
It is common practice to split long time series into sub-sequences, treating them as independent samples. The simplest way to do this is, to slide a window of length <code>SEQ_LEN</code> through each series and treat each subseries contained in the window as one sample. The direct successor after the window defines the respective label for the sub-series (Image 4).
Note that we will use the terms sample sequence and sub-sequence to a certain degree interchangable. Sample is used refering to the sequential counts in a row of the data matrix used for model training. Sub-sequence also refers to such a row, but it shall emphasis the row beeing a sub-sequence of a longer time series over the whole observation period.</p>
<p style='text-align: center;'>
<img align="center" src="/blog/img/seminar/lstm_gru_1819/Format1.png" alt="RNN input format" style="width: 450px;"/><br>
Image 4: Formatting data input for recurrent neural networks.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># changing long into wide format</span>
<span class="n">full_seqs</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;District&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;Date&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">&#39;Incidents&#39;</span><span class="p">)</span>

</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span><span class="c1"># split data on date into train and validation set</span>
</span><span class="n">dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">full_seqs</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">lower</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2015-01-01&quot;</span><span class="p">)</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2017-01-01&quot;</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2018-01-01&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tr_idx</span> <span class="o">=</span> <span class="n">full_seqs</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">dates</span> <span class="o">&lt;</span> <span class="n">lower</span><span class="p">]</span>
<span class="n">val_idx</span> <span class="o">=</span> <span class="n">full_seqs</span><span class="o">.</span><span class="n">columns</span><span class="p">[(</span><span class="n">dates</span> <span class="o">&gt;=</span><span class="n">lower</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">dates</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">)]</span>

<span class="n">full_seqs_tr</span> <span class="o">=</span> <span class="n">full_seqs</span><span class="p">[</span><span class="n">tr_idx</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">full_seqs_val</span> <span class="o">=</span> <span class="n">full_seqs</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">full_seqs_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">full_seqs_val</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[8]:</div>





<div class="output_text output_subarea output_execute_result">
<pre>((24, 5113), (24, 731))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We implement the sliding window by looping through the 24 full sequences and use array slicing withn a list comprehension to create the sub-sequences. The resulting sub-sequences are stacked vertically. Recurrent layers in keras and <code>MyRNN</code> above expect an input tensor of the dimensions <code>[batch_size, timesteps, input_dim]</code>. Since we work with one dimensional series, we have ignored the third dimension so far. In order to match the required input shape a thrid dimension of size one is added to the array.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">cut_sequences</span><span class="p">(</span><span class="n">long_seqs</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">):</span>
  <span class="c1"># input matrix of long sequences [sequences, timesteps]</span>
  <span class="c1"># returns matrix of subsequences X [subsequences, SEQ_LEN, 1], and labels y[subsequences,1]</span>
  <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">long_seq</span> <span class="ow">in</span> <span class="n">long_seqs</span><span class="p">:</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">long_seq</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># slide window of SEQ_LEN over full sequence</span>
    <span class="n">seq_stacked</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">long_seq</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">SEQ_LEN</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">SEQ_LEN</span><span class="p">)])</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">long_seq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">n</span><span class="p">)])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_stacked</span><span class="p">)</span>
    <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

  <span class="n">X</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="c1">#add axis for number of features per time step = 1  </span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="n">y</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

  <span class="k">return</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># cut sequences in sub sequences of length SEQ_LEN</span>
<span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="n">cut_sequences</span><span class="p">(</span><span class="n">full_seqs_tr</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">cut_sequences</span><span class="p">(</span><span class="n">full_seqs_val</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">)</span>

<span class="n">X_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_tr</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[10]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>((122472, 10, 1), (122472, 1), (17304, 10, 1), (17304, 1))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluation-metric-and-benchmark">Evaluation metric and benchmark<a class="anchor-link" href="#Evaluation-metric-and-benchmark">&#182;</a></h3><p>Before training a model one should consider how the prediction error may be evaluated w.r.t. the application objective. We use the mean squared error (MSE) as loss function, which is common for regression tasks.
For user of the model, MSE is hard to interpret in the context of the model application. The mean absolute error (MAE) seems more interpretable: In combination with average crime counts it provides some intuition about the uncertainty included in model predictions. To get a basic feeling for model performance while building the model we establish as naive benchmark: We calculate the MAE using the last observed crime count as prediction for the succeeding crime count. This results in a MAE of 6.65.  Given the mean of 46 crimes per day and district this does not seem to bad for  a simple heuristic.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># naive benchmark: predict last observed count</span>
<span class="n">AVG</span> <span class="o">=</span> <span class="n">y_tr</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">full_seqs_val</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">full_seqs_val</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">MAE_last_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">preds</span> <span class="o">-</span> <span class="n">y_true</span> <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average daily crime count by district: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">AVG</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MAE predict last value: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">MAE_last_val</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Average daily crime count by district: 46
MAE predict last value: 6.652
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training">Training<a class="anchor-link" href="#Training">&#182;</a></h3><p>Like other neural networks, RNNs are trained trough back propagation. To derive the gradient for an RNN one must calculate the impact on the loss considering all inputs back to the first time step. This is called backpropagation through time (bptt). We look in more detail into the derivation when introducing the vanishing gradient problem and deriving the gradient for RNNs with Long-Short-Term Memory. Here we use the keras <code>.compile</code> method to pass the loss function, the optimizer and MAE as evaluation metric to <code>myRNN</code>.  <br>
We then train <code>myRNN</code>on the described input format. Subsequently we demonstrate a different data format using input and output sequences. We train two new models, one with and one without the <code>stateful</code> property. Subsequently we compare their training time and behaviour. Therfore we fit each model for 50 epochs, they converge relativ quickly, and save the results to a <code>Histroy</code> object. We focus on the concepts behind the models and their implementation. To keep things simple we use the same hyperparameter set for every model and briefly evaluate the MAE on the validation set over different epochs. We will look into more detail into hyperparameter tuning after introducing more complex RNNs with LSTM and GRU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">B_SZ</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">EP</span> <span class="o">=</span>  <span class="mi">50</span>

<span class="n">myRNN</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mae&#39;</span><span class="p">])</span>
<span class="n">myRNN_hist</span> <span class="o">=</span> <span class="n">myRNN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">get_input</span><span class="p">(</span><span class="n">X_tr</span><span class="p">),</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">B_SZ</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="n">EP</span> <span class="p">,</span> <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">get_input</span><span class="p">(</span><span class="n">X_val</span><span class="p">),</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Alternative-formatting-with-prediction-sequence">Alternative formatting with prediction sequence<a class="anchor-link" href="#Alternative-formatting-with-prediction-sequence">&#182;</a></h3><p>The way we created sample sequences so far, seems quite inefficient in terms of data usage: By sliding a window one step at time through the sequences we create mostly redundant sample sequences. A more efficient way of presenting data to the model is illustrated in Image 6. Instead of changing one value in each sample sequence, they are now defined by disjunct sub-sequences of lenght <code>SEQ_LEN</code>. Instead of predicting one label after observing a complete sub-sequence, we assigne a corresponding label sequences of the same length to each sample sequence. Each label sequence starts one time step ahead of the sample sequence.
The remaining values, i.e. the remainder of the full sequence length divided by <code>SEQ_LEN</code>, are dropped. This reduced the size of the data set by a factor of <code>SEQ_LEN</code>, without losing any information compared to the previous formatting.</p>
<p style='text-align: center;'><img align="center" src="/blog/img/seminar/lstm_gru_1819/Format2.png" alt="RNN format return sequences" style="width: 450px;"/><br>
Image 5: Input format with prediction sequence.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">cut_sequence_return_state</span><span class="p">(</span><span class="n">long_seqs</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">cut_seq_start</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
  <span class="c1"># input matrix of long sequences [sequences, timesteps]</span>
  <span class="c1"># returns matrix of disjunct subsequences X [subsequences, SEQ_LEN, 1], and labels y[subsequences,SEQ_LEN,1]</span>
  <span class="c1"># output is ordered first by time step then by district</span>

  <span class="n">long_seqs_X</span> <span class="o">=</span> <span class="n">long_seqs</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">long_seqs_y</span> <span class="o">=</span> <span class="n">long_seqs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span>
  <span class="k">if</span><span class="p">(</span><span class="n">cut_seq_start</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">long_seqs_X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">SEQ_LEN</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">long_seqs_X</span><span class="p">[:,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">SEQ_LEN</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="n">long_seqs_X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">SEQ_LEN</span><span class="p">)]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">long_seqs_y</span><span class="p">[:,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">SEQ_LEN</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="n">long_seqs_y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">SEQ_LEN</span><span class="p">)]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">long_seqs_X</span><span class="p">[:,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">SEQ_LEN</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="n">long_seqs_X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">SEQ_LEN</span><span class="p">)]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">long_seqs_y</span><span class="p">[:,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">SEQ_LEN</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="n">long_seqs_y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">SEQ_LEN</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">SEQ_LEN</span><span class="p">:</span>
      <span class="n">X</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
      <span class="n">y</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>

  <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">y</span><span class="p">)[:,:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span> <span class="o">=</span> <span class="n">cut_sequence_return_state</span><span class="p">(</span><span class="n">full_seqs_tr</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">cut_sequence_return_state</span><span class="p">(</span><span class="n">full_seqs_val</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">cut_seq_start</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now replace <code>myRNN</code> by the keras <code>SimpleRNN()</code> layer in combination with a <code>Dense()</code> output layer. While this implements the same behaviour as <code>myRNN</code> it allows for additional functionality. By setting the <code>return_sequence</code> parameter to true it passes all hidden states, not only the last one, to the following layer. In the <code>myRRN</code> code above, this could have been implemented, by appending all hidden states to a list instead of updating the variable <code>ht</code>. The hidden states within the list are then passed through the output layer. The outputlayer returns for every time step in this hidden state sequence a prediction for the following time step in the label sequence. To implement this above, the output layer may be included in de loop (Image 3). The predictions then need to be save in a new output list. In keras this can be done more conviniently, by wrapping the output layer in the <code>TimeDistributed()</code> Wrapper. Image 6 shows the unrolled illustration of an RNN predicting sequences.</p>
<p style='text-align: center;'><img align="center" src="/blog/img/seminar/lstm_gru_1819/RNN_Ret_Seq.png" alt="RNN return sequences" style="width: 450px;"/><br>
Image 6: Recurrent neural network with return sequence</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># construct and train SimpleRNN with return sequences</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="n">H_SZ</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">SEQ_LEN</span><span class="p">,</span><span class="n">FEAT_SZ</span><span class="p">),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">FEAT_SZ</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mae&#39;</span><span class="p">])</span>
<span class="n">seqRNN_hist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">B_SZ</span><span class="p">,</span>  <span class="n">epochs</span> <span class="o">=</span> <span class="n">EP</span><span class="p">,</span> <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stateful-recurrent-neural-networks">Stateful recurrent neural networks<a class="anchor-link" href="#Stateful-recurrent-neural-networks">&#182;</a></h3><p>When comparing Image 6 and Image 2 you may notice a disadvatage of the return sequence formatting: In both cases, when making a prediction for $y_{t+1}$ the network has seen the inputs $x_{t-2}$, $x_{t-1}$ and $x_{t}$. When using a return sequences to make a prediction for $y_{t-1}$ in Image 6 the only network input is $x_{t-2}$. The hidden state has been initialized with zeros. Therefore we might suspect the error for the predictions on the beginning of the sequence to be higher than at the end. This problem occurs because we need to cut the full sequences into sub-sequences treated as independet samples. One way to avoid that is by saving the last hidden state of one sub-sequence and use it as inizialization of $h_{t}$ when passing the following subsequence (from the initial full sequence) through the network. In keras this can be done by setting the <code>stateful</code> parameter of the RNN to true. To memorize which subsequences belongt to the same district the batch size is used. It needs to be set equal to the number of independen full series, e.g here the number of districts. Note that the return sequence formatting is ordered by time and district: The first sub-sequence of the first district is in the first row and the first sub-sequence of the second district in the second row. The 25th row contains the second sub-sequence of the first district and so on. The samples within on batch get passed through the network in paralell. Thus the last hidden state of one row index in a batch is used to initialize the initial hidden state of the same row index in the next batch. When calling the <code>.fit</code> method keras shuffles the rows in the training data by defaul at the beginning of every training epoch. To make use of the stateful property the <code>shuffle</code> is set to false. Moreover the hidden states needs to be reset after every epoch. Otherwise the hidden state of the last time step of the full sequence would be passed on to the first time step at the next epoch. To do this we train the network in a for loop one epoch at a time and reset the state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># construct stateful RNN</span>
<span class="n">B_SZ</span> <span class="o">=</span> <span class="mi">24</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">SimpleRNN</span><span class="p">(</span><span class="n">H_SZ</span><span class="p">,</span> <span class="n">batch_input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">B_SZ</span><span class="p">,</span> <span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">FEAT_SZ</span><span class="p">),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stateful</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">FEAT_SZ</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mae&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># train stateful RNN append epoch metrics and loss to list</span>
<span class="n">stateful_val_mae</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">stateful_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">stateful_val_loss</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EP</span><span class="p">):</span>
    <span class="n">stateRNN_hist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">B_SZ</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
    <span class="n">stateful_val_mae</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stateRNN_hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_mean_absolute_error&#39;</span><span class="p">])</span>
    <span class="n">stateful_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stateRNN_hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
    <span class="n">stateful_val_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stateRNN_hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Comparing-training-behaviour">Comparing training behaviour<a class="anchor-link" href="#Comparing-training-behaviour">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When training a stateful RNN it is important to understand that the value of the last state is simply copied to the initial hidden state of the next subsequence. This may improve the prediction error, but when deriving the gradient, they are treated as constant. The inputs considered when updating the weights are still restricted to the sub-sequence length. Therefore, <a href="https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html">some practitioners</a> argue that "unstateful” RNNs achieve in many application better results. Reasons for this include the loss of randomness by stopping to shuffle the data between epochs and the fixed batch size, which is a parameter to which RNNs might be sensitive.
<a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/">Brownlee</a> gives as a rule of thumb to use stateful RNNs when the output mainly depends on the occurence of a certain input. This may be the case in many natural language tasks. If the outputs represent a complex function of the previous time steps, increasing the subsequence length may be necessary instead. We will evaluate the behaviour of the MAE for the three models on the validation set over the training epochs. <br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># plot MAE over epochs for different models</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">MAE_last_val</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">myRNN_hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_mean_absolute_error&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seqRNN_hist</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_mean_absolute_error&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stateful_val_mae</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Mean absolut error on validation set&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MAE&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">20</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="c1"># (w, h)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;predict last count&#39;</span><span class="p">,</span> <span class="s1">&#39;myRNN&#39;</span><span class="p">,</span> <span class="s1">&#39;returnSeq&#39;</span><span class="p">,</span> <span class="s1">&#39;statefulRNN&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first thing to notice is, that <code>myRRN</code> converges clearly faster than the two other models. It reaches already after the first epoch an MAE around six. This makes sense because the size of the training set was ten times bigger. However the <code>seqRNN</code> has not converged after ten epochs. So basically we make up for the reduced redundancy in the new data format by training for more epochs.
All models perform better than the naive benchmark of 6.65. However, the improvment seems moderate given the additional model complexity. <code>seqRNN</code> and <code>statefulRNN</code> seem to converge against a value above five, while <code>seqRNN</code> only reaches a value close to six. The higher value of <code>statefulRNN</code> was expected due to the missing information at the start of the sample sequences. The stateful property could reduce the MAE back to the level of <code>myRNN</code> but it does not clearly improve it. Due to this result and the additional restrictions when building a stateful model we continue to work with the "stateless" variant and the first data format.<br>
So far we have been using a relativ small <code>SEQ_LEN</code> of ten. A simpler way of providing more information on past observations than the stateful option would be to increase the sample sequence length. We have not considered that so far because simple RNNs are hard to train on long sequences. The reason for this is the vanishing gradient problem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Vanishing-Gradient-Problem">The Vanishing Gradient Problem<a class="anchor-link" href="#The-Vanishing-Gradient-Problem">&#182;</a></h3><p>Even though RNNs cover the memory problem of simple neural networks, in practice we still face issues. The introduced Simple RNNs do have problems with long-term dependencies which they seem to forget.</p>
<p>Let us have a look at the following sentence as a sequence.</p>
<h5 id="&#8220;In-France,-I-had-a-great-time-and-I-learnt-some-of-the-...?-[language]&quot;">&#8220;In France, I had a great time and I learnt some of the ...? [language]"<a class="anchor-link" href="#&#8220;In-France,-I-had-a-great-time-and-I-learnt-some-of-the-...?-[language]&quot;">&#182;</a></h5><p>In this case, we would need the word "France" to remember to later on predict the word "French" at the end of the sentence. RNNs will have problems to still refer to the word "France" although they consider the previous timesteps in the current one. This problem refers to the so called vanishing (or exploding) gradient problem which is captured in the formulas below. In practice this means that crucial information of very far back timesteps do not have an influence on the current or future timesteps anymore or that irrelevant information gets too much influence.</p>
<p><img src="/blog/img/seminar/lstm_gru_1819/vanishing_gradient.png" alt="vanishing_gradient" style="width: 600px;"/>
<br></p>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">(Olah, 2015)</a></p>
<p>During backpropagation through time the gradient of the loss function can be represented as follows.</p>
$$\frac{\partial J_t}{\partial W_{in}} =\frac{\partial J_t}{\partial y_{t}}\frac{\partial y_t}{\partial h_{t}}\frac{\partial h_t}{\partial h_{t-1}}
\frac{\partial h_{t-1}}{\partial h_{t-2}}\space\space ...\frac{\partial h_{0}}{\partial W_{in}}$$<p>It can be shown that as the gap between the timesteps gets bigger the product of the gradients of the hidden states w.r.t. the previous states
$$\frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\frac{\partial h_{t-2}}{\partial h_{t-3}}\space\space...$$
<br>
gets longer and longer and a lot of very small numbers are multiplied. This can also be seen in the following equation. Since the weights $W_in$ are sampled from a standard normal distribution this term will mostly be smaller than one. Similarly, the absolute value of the tanh will also be between zero and one. <a href="http://introtodeeplearning.com/2017/Sequence%20Modeling.pdf">(Suresh, 2017)</a></p>
$$\frac{\partial h_t}{\partial h_{t-1}}
= W_{in}^T diag[tanh^{ `}(W_{in}+W_h x_j)]$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Literature-Review">Literature Review<a class="anchor-link" href="#Literature-Review">&#182;</a></h3><p>As suggested by <a href="https://www.econstor.eu/bitstream/10419/157808/1/886576210.pdf">(Fischer et. al, 2017)</a> one can make good use of an LSTM network for financial market prediction since it is a state-of-the art technique for sequence learning. Even though this is not the same domain as our crime use case, the authors of the paper still demonstrate that LSTM networks can extract relevant information from the underlying noisy data. The LSTM network was primarily introduced by <a href="https://www.bioinf.jku.at/publications/older/2604.pdf">(Hochreiter and Schmidhuber, 1997)</a> to overcome the vanishing gradient problem and to learn long-term dependencies since the simple RNN is not capable of doing that. We know that predicting crime rates is not the typical use case for introducing LSTM and GRU networks. Nevertheless, our crime data is also sequential since we are looking at a certain period of time to predict the future crime incidences and in addition, we expect to need the capability of learning long-term dependencies. Furthermore, there have already been some successful approaches forecasting crime with LSTM networks <a href="https://arxiv.org/pdf/1806.01486.pdf">(Stec et. al, 2018)</a>. The Gated Recurrent Unit network, which is also a Recurrent Neural Network, is also getting more and more popular for the task of prediction when having sequential data. It was only introduced a few years ago <a href="https://arxiv.org/pdf/1406.1078.pdf">(Cho et. al, 2014)</a> and it has been shown that the GRU performs similar or even better, depending on the data and the setup. For instance, traffic flow has been predicted by an LSTM and a GRU network whereby the GRU performed slightly better than the LSTM model <a href="https://www.researchgate.net/profile/Li_Li240/publication/312402649_Using_LSTM_and_GRU_neural_network_methods_for_traffic_flow_prediction/links/5c20d38d299bf12be3971696/Using-LSTM-and-GRU-neural-network-methods-for-traffic-flow-prediction.pdf">(Fu et. al,  2016)</a>. That is why usually both models are built to see which one performs better in the given use case
<a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">(Nguyen, 2018)</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Advanced-RNNs:-LSTMs-and-GRUs">Advanced RNNs: LSTMs and GRUs<a class="anchor-link" href="#Advanced-RNNs:-LSTMs-and-GRUs">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/overview_RNN_LSTM_GRU.png" alt="LSTM" style="width: 800px;"/></p>
<p><a href="https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464">(Tch, 2017)</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As already suggested, there are more advanced solutions than the simple RNN for the prediction of the next step in a sequence. To overcome the problem of the vanishing (and exploding) gradient, i.e. having crucial information in a very far back timestep that might vanish or irrelevant information might happen to have too much influence on the current time step, we suggest to use LSTM (Long Short-Term Memory) and the GRU (Gated Recurrent Unit) networks. In the following, we will tackle these popular approaches. Instead of the RNN cells the LSTM and GRU have more complex cells in the hidden states. The idea is to control the information flow and regulate what is passed through and what not and decide what the relevant information is from the current as well as the previous timestep. This is done by the different gates in the gated cells.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="LSTM-Networks---Long-Short-Term-Memory-Networks">LSTM Networks - Long Short-Term Memory Networks<a class="anchor-link" href="#LSTM-Networks---Long-Short-Term-Memory-Networks">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/LSTM.png" alt="LSTM" style="width: 800px;"/></p>
<p>The LSTM is a special type of RNN which can learn long-term dependencies. Instead of having only one tanh layer as in the RNN, more layers as can be seen as the yellow rectangles. The LSTM has 3 gates which are represented as the sigmoid layers. The output of sigmoid function is alsways something in between zero and one, i.e. to decide if information is let through or not. We will refer to Christopher Olah's graphics and explanations in this part <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">(Olah, 2015)</a>. Let us walk you through the complex LSTM cell step by step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/img/seminar/lstm_gru_1819/lstm_core_idea.png" alt="LSTM" style="width: 400px;"/></p>
<p>There is always a horizontal line running through all cells between the previous cell state $C_t-1$ and the current cell state $C_t$ and in between there happen a few interactions. A very important fact about the LSTM cell is that there is the cell state on the one hand which is often referred to as a memory. On the other hand, there is also a hidden state which is often referred to as the output. One can maybe think of the hidden state $h_t$ as a filtered version of the cell state, whereas the cell state has the ability to hold on to information that is not necessary for the current output. For instance, when we want to output the next word, we might not need all the information of this word to output but maybe we want to save the information of this word, such as if it is singular or plural or a noun or a verb, to predict words in a future timestep.
Therefore, the cell state can hold on to information that might be useful in upcoming timesteps.</p>
<p>The key things for now are that we have three gates in the LSTM cell and that the hidden state is separate from the cell state. Furthermore, by the gates information can be removed or added from the cell state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize sample data and weights</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">7.0</span>

<span class="n">Wi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">3.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]])</span>
<span class="n">Ui</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">]])</span>

<span class="n">Wf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">]])</span>
<span class="n">Uf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">3.6</span><span class="p">,</span><span class="mf">4.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.9</span><span class="p">]])</span>

<span class="n">Wo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.1</span><span class="p">]])</span>
<span class="n">Uo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">4.3</span><span class="p">]])</span>

<span class="n">Wc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">]])</span>
<span class="n">Uc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.8</span><span class="p">,</span><span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.7</span><span class="p">,</span><span class="mf">2.9</span><span class="p">]])</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">])</span>

<span class="n">h0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Forget-Gate">The Forget Gate<a class="anchor-link" href="#The-Forget-Gate">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/lstm_forget.png" alt="LSTM" style="width: 700px;"/></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, there is a forget gate which decides what information to forget or even to leave everything through as it is.
As input the output of the previous cell $h_t-1$ and input of the current timestep are fed into the cell. The output will be something in between zero and one, i.e. one means that everything is kept from the previous timestep whereas zero means that everything from the previous timestep will be forgotten.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inner_f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wf</span><span class="p">,</span><span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span><span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Uf</span><span class="p">,</span><span class="n">h0</span><span class="p">)</span>

<span class="n">f1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_f1</span><span class="p">))</span>
<span class="n">f1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[4]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.61301418, 0.50999867])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Input-Gate">The Input Gate<a class="anchor-link" href="#The-Input-Gate">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/lstm_input.png" alt="LSTM" style="width: 700px;"/></p>
<p>In the next step, it will be decided selectively what information to update in the cell state. This is done in two steps. First, it is determined which values to update and second, the vector with the values of the new candidates for the cell state is calculated.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inner_i1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wi</span><span class="p">,</span><span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ui</span><span class="p">,</span><span class="n">h0</span><span class="p">)</span>

<span class="n">i1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_i1</span><span class="p">))</span>
<span class="n">i1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[5]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.65021855, 0.50499983])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inner_c1_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wc</span><span class="p">,</span> <span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Uc</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>

<span class="n">c1_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">inner_c1_tilde</span><span class="p">)</span>
<span class="n">c1_tilde</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[6]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.03997868, 0.07982977])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Update-of-the-Cell-State">The Update of the Cell State<a class="anchor-link" href="#The-Update-of-the-Cell-State">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/lstm_new_cell_state.png" alt="LSTM" style="width: 600px;"/></p>
<p>Now, the new cell state can be computed as follows. The cell state of the previous timestep is multipliead by the values we want to forget, i.e. this first term of the equation gives us the part, that we want to remember from the previous timestep. Then, one adds the proportion determined by the input gate (of what we want to update) of the values of the new candidates. Basically, this is about how much is kept from the previous cell state and how much we take from the candidates for the new cell state to actually calculate the cell state of timestep t.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">c0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">])</span>

<span class="n">c1</span> <span class="o">=</span> <span class="n">f1</span> <span class="o">*</span> <span class="n">c0</span> <span class="o">+</span> <span class="n">i1</span> <span class="o">*</span> <span class="n">c1_tilde</span>
<span class="n">c1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[7]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.02599488, 0.04031402])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Output-Gate">The Output Gate<a class="anchor-link" href="#The-Output-Gate">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/lstm_output.png" alt="LSTM" style="width: 600px;"/></p>
<p>So what is the ouput of the cell? Basically, it is a filtered version of the cell state. The sigmoid layer (the output gate) filters what parts of the cell state will be included in the output $h_t$. The cell state is put through a tanh layer and ultimately multiplied by the values of the output gate. Thus, the output is a filtered version of the cell state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># outpute gate</span>
<span class="n">inner_o1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wo</span><span class="p">,</span> <span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Uo</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>

<span class="n">o1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_o1</span><span class="p">))</span>
<span class="n">o1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[8]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.50499983, 0.65021855])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h1</span> <span class="o">=</span> <span class="n">o1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c1</span><span class="p">)</span>
<span class="n">h1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[9]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.01312445, 0.02619873])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, that we know how to calculate all the values of one time step in an LSTM cell, we can just plug in the new updated values of the input $x_t$, the hidden state $h_{t-1}$ and the cell state $c_t$ for the second and third time step into the same equations. Usually, one would implement this in a loop, but for the purpose of demonstration we show you the steps in detail.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Second-time-step">Second time step<a class="anchor-link" href="#Second-time-step">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># forget gate </span>
<span class="n">inner_f2</span> <span class="o">=</span> <span class="n">Wf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
<span class="n">f2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_f2</span><span class="p">))</span>

<span class="c1"># input gate</span>
<span class="n">inner_i2</span> <span class="o">=</span> <span class="n">Wi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span> <span class="o">+</span> <span class="n">Ui</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
<span class="n">i2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_i2</span><span class="p">))</span>

<span class="n">inner_c2_tilde</span> <span class="o">=</span> <span class="n">Wc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span> <span class="o">+</span> <span class="n">Uc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
<span class="n">c2_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">inner_c2_tilde</span><span class="p">)</span>

<span class="c1"># update of the cell state</span>
<span class="n">c2</span> <span class="o">=</span> <span class="n">f2</span> <span class="o">*</span> <span class="n">c1</span> <span class="o">+</span> <span class="n">i2</span> <span class="o">*</span> <span class="n">c2_tilde</span>

<span class="c1"># output gate</span>
<span class="n">inner_o2</span> <span class="o">=</span> <span class="n">Wo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span> <span class="o">+</span> <span class="n">Uo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
<span class="n">o2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_o2</span><span class="p">))</span>

<span class="n">h2</span> <span class="o">=</span> <span class="n">o2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c2</span><span class="p">)</span>
<span class="n">h2</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[10]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.07524102, 0.11116973])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Third-time-step">Third time step<a class="anchor-link" href="#Third-time-step">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># forget gate </span>
<span class="n">inner_f3</span> <span class="o">=</span> <span class="n">Wf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
<span class="n">f3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_f3</span><span class="p">))</span>

<span class="c1"># input gate</span>
<span class="n">inner_i3</span> <span class="o">=</span> <span class="n">Wi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span> <span class="o">+</span> <span class="n">Ui</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
<span class="n">i3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_i3</span><span class="p">))</span>

<span class="n">inner_c3_tilde</span> <span class="o">=</span> <span class="n">Wc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span> <span class="o">+</span> <span class="n">Uc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
<span class="n">c3_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">inner_c3_tilde</span><span class="p">)</span>

<span class="c1"># update of the cell state</span>
<span class="n">c3</span> <span class="o">=</span> <span class="n">f3</span> <span class="o">*</span> <span class="n">c2</span> <span class="o">+</span> <span class="n">i3</span> <span class="o">*</span> <span class="n">c3_tilde</span>

<span class="c1"># output gate</span>
<span class="n">inner_o3</span> <span class="o">=</span> <span class="n">Wo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span> <span class="o">+</span> <span class="n">Uo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
<span class="n">o3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inner_o3</span><span class="p">))</span>

<span class="n">h3</span> <span class="o">=</span> <span class="n">o3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c3</span><span class="p">)</span>
<span class="n">h3</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[11]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.28170128, 0.37065888])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prediction">Prediction<a class="anchor-link" href="#Prediction">&#182;</a></h3><p>After running through all the time steps, we predict our target variable.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h3</span><span class="p">)</span>
<span class="n">y_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[12]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>2.046038096901425</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="GRU-Networks---Gated-Recurrent-Unit-Networks">GRU Networks - Gated Recurrent Unit Networks<a class="anchor-link" href="#GRU-Networks---Gated-Recurrent-Unit-Networks">&#182;</a></h3><p><img src="/blog/img/seminar/lstm_gru_1819/GRU.png" alt="GRU" style="width: 600px;"/></p>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">(Olah, 2015)</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The GRU network is a variation of the LSTM network and looks similar. Instead of having three gates, it only has two gates, the update and the reset gate which also work similar to the gates in the LSTM cell.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize sample data and weights</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">7.0</span>

<span class="n">Wz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.1</span><span class="p">]])</span>
<span class="n">Uz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">4.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]])</span>

<span class="n">Wr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]])</span>
<span class="n">Ur</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.3</span><span class="p">,</span><span class="mf">7.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">9.1</span><span class="p">,</span><span class="mf">4.5</span><span class="p">]])</span>

<span class="n">Wh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]])</span>
<span class="n">Uh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">2.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span><span class="mf">3.6</span><span class="p">]])</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As input, the cell gets the input of the current timestep and the previous output $h_t-1$. An important difference to the LSTM is that there is no separate cell state: the hidden state and cell state are kind of "merged" so that there only remains a hidden state $h_t$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Update-Gate">Update Gate<a class="anchor-link" href="#Update-Gate">&#182;</a></h3><p>In the update gate $z_t$ it is determined what values to update from the past and the reset gate decides what information should be reset, i.e. removed from the past.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">])</span>

<span class="n">z1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">Wz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h0</span><span class="p">))))</span>
<span class="n">z1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[14]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.50499983, 0.65021855])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Reset-gate">Reset gate<a class="anchor-link" href="#Reset-gate">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">r1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">Wr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span><span class="o">+</span> <span class="n">Ur</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h0</span><span class="p">))))</span>
<span class="n">r1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[15]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.61301418, 0.52497919])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="New-candidates-for-current-hidden-state">New candidates for current hidden state<a class="anchor-link" href="#New-candidates-for-current-hidden-state">&#182;</a></h3><p>The new candidates for the current cell state are calculated by taking information from the reset gate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h1_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">0</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r1</span><span class="o">*</span> <span class="n">h0</span><span class="p">))</span>
<span class="n">h1_tilde</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[16]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.03997868, 0.17808087])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="New-hidden-state:-the-output">New hidden state: the output<a class="anchor-link" href="#New-hidden-state:-the-output">&#182;</a></h3><p>The new output $h_t$ is defined by the proportion $1-z_t$ of the previous hidden state $h_t-1$ and by the proportion $z_t$ of the new candidates for the current hidden state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">z1</span><span class="p">)</span><span class="o">*</span><span class="n">h0</span> <span class="o">+</span> <span class="p">(</span><span class="n">z1</span><span class="o">*</span><span class="n">h1_tilde</span><span class="p">)</span>
<span class="n">h1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[17]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.02018923, 0.11579148])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, we plug in all the updated values for the next time steps into the same equations like we did for the LSTM forward pass.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Second-time-step">Second time step<a class="anchor-link" href="#Second-time-step">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">z2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">Wz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">))))</span>

<span class="n">r2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">Wr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span><span class="o">+</span> <span class="n">Ur</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">))))</span>

<span class="n">h2_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">1</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r2</span><span class="o">*</span> <span class="n">h1</span><span class="p">))</span>

<span class="n">h2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">z2</span><span class="p">)</span><span class="o">*</span><span class="n">h1</span><span class="o">+</span> <span class="p">(</span><span class="n">z2</span><span class="o">*</span><span class="n">h2_tilde</span><span class="p">)</span>
<span class="n">h2</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[18]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.18717827, 0.42379445])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Third-time-step">Third time step<a class="anchor-link" href="#Third-time-step">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">z3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">Wz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h2</span><span class="p">))))</span>

<span class="n">r3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">Wr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span><span class="o">+</span> <span class="n">Ur</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h2</span><span class="p">))))</span>

<span class="n">h3_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">]])</span><span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r3</span><span class="o">*</span> <span class="n">h2</span><span class="p">))</span>

<span class="n">h3</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">z3</span><span class="p">)</span><span class="o">*</span><span class="n">h2</span><span class="o">+</span> <span class="n">z3</span><span class="o">*</span><span class="n">h3_tilde</span>
<span class="n">h3</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[19]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([0.79220277, 0.8899337 ])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prediction">Prediction<a class="anchor-link" href="#Prediction">&#182;</a></h3><p>After running through all the time steps, we predict our target variable.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h3</span><span class="p">)</span>
<span class="n">y_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[20]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>5.144140350766751</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">tanh</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Backpropagation-LSTM">Backpropagation LSTM<a class="anchor-link" href="#Backpropagation-LSTM">&#182;</a></h3><p>$
% Forget-Gate
\newcommand{\F}{\color{orange}{f_t}}
\newcommand{\Ffull}[1][]{\color{orange}{\sigma^{#1} \left( W_f + U_f \cdot h_{t-1} \right)}}
\newcommand{\f}[1]{\color{orange}{f^t_{#1}}}
\newcommand{\ffull}[2][]{\color{orange}{\sigma^{#1} \left(w^f_{#2} x_t + u^f_{#2 1} h^{t-1}_1 + u^f_{#2 2}h^{t-1}_2 \right)}}
\newcommand{\Wf}{\color{orange}{W_f}}
\newcommand{\Uf}{\color{orange}{U_f}}
\newcommand{\wf}[1]{\color{orange}{w_{#1}^f}}
\newcommand{\uf}[2]{\color{orange}{u_{#1 #2}^f}}
% Input-Gate
\newcommand{\I}{\color{red}{i_t}}
\newcommand{\Ifull}[1][]{\color{red}{\sigma^{#1} \left( W_i + U_i \cdot h_{t-1} \right)}}
\newcommand{\i}[1]{\color{red}{i^t_{#1}}}
\newcommand{\ifull}[2][]{\color{red}{\sigma^{#1} \left( w^i_{#2} x_t + u^i_{#2 1} h^{t-1}_1 + u^i_{#2 2} h^{t-1}_2 \right)}}
\newcommand{\Wi}{\color{red}{W_i}}
\newcommand{\Ui}{\color{red}{U_i}}
\newcommand{\wi}[1]{\color{red}{w_{#1}^i}}
\newcommand{\ui}[2]{\color{red}{u_{#1 #2}^i}}
% Output-Gate
\newcommand{\O}{\color{blue}{o_t}}
\newcommand{\Ofull}[1][]{\color{blue}{\sigma^{#1} \left( W_o + U_o \cdot h_{t-1} \right)}}
\newcommand{\o}[1]{\color{blue}{o^t_{#1}}}
\newcommand{\ofull}[2][]{\color{blue}{\sigma^{#1} \left( w^o_{#2} x_t + u^o_{#2 1} h^{t-1}_1 + u^o_{#2 2} h^{t-1}_2 \right)}}
\newcommand{\Wo}{\color{blue}{W_o}}
\newcommand{\Uo}{\color{blue}{U_o}}
\newcommand{\wo}[1]{\color{blue}{w_{#1}^o}}
\newcommand{\uo}[2]{\color{blue}{u_{#1 #2}^o}}
% Cell (c without tilde - or however the fuck it's supposed to be called)
\newcommand{\C}[1][]{\color{lime}{c_{t #1}}}
\newcommand{\Cfull}{\F * \C[-1] + \I * \Ctil}
\newcommand{\c}[2][]{\color{lime}{c^{t #1}_{#2}}}
% Candidate (c with tilde - again: whatever, it's fine.)
\newcommand{\Ctil}{\color{green}{\tilde{c}_t}}
\newcommand{\Ctilfull}[1][]{\color{green}{g^{#1} \left( W_c x_t + U_c \cdot h_{t-1} \right)}}
\newcommand{\ctil}[2][]{\color{green}{g^{#1} \left( w^c_{#2} x_t + u^c_{#2 1} h^{t-1}_1 + u^c_{#2 2} h^{t-1}_2 \right)}}
\newcommand{\ctils}[1]{\color{green}{\tilde{c}_{#1}}}
\newcommand{\Wc}{\color{green}{W_c}}
\newcommand{\Uc}{\color{green}{U_c}}
\newcommand{\wc}[1]{\color{green}{w_{#1}^c}}
\newcommand{\uc}[2]{\color{green}{u_{#1 #2}^c}}
% Miscellaneous
\newcommand{\yhat}{\hat{y}_t}
\newcommand{\error}{\left(y - \yhat \right)}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dxt}[1]{\color{#1}{x_t}}
\DeclareMathOperator{\diag}{diag}
\newcommand{\myH}[2][-1]{\color{#2}{h_{t #1}}}
\newcommand{\myh}[2][red]{\color{#1}{h^{t-1}_{#2}}}
$
In order to demonstrate how the backpropagation on the LSTM works we will further investigate our previous example. Remember, that we used a $3 \times 1$ vector for our single sample. Additionally, we looked at a very simple neural network that has only 2 neurons. Typically, we want to find better weights in each consecutive training iteration and in order to achieve that we follow a fraction of the negative gradient of a loss function.</p>
<p>So right now, we need two things:</p>
<ol>
<li>a loss function</li>
<li>and its gradients with respect to each of the weight matrices.</li>
</ol>
<p>Regarding the loss function we can make our lives easier by picking one, that is fit for our regression task while also being easy to derive. One of the easiest loss functions for that matter is the sum of squared errors, which we scale by a convenient constant:</p>
$$loss_t = \frac{1}{2}\left( y - \yhat \right)^2.$$<p>Where $\yhat$ is the prediction at timestep $t$. When taking the derivative with respect to $\yhat$ one can easily see:</p>
$$\deriv{loss_t}{\yhat} = \left( y - \yhat \right) (-1)$$<p>We define our total loss to be the sum of the losses at each timestep:</p>
$$loss = \sum loss_t$$<p>which of course gives</p>
$$\deriv{loss}{\yhat} = \sum\deriv{loss_t}{\yhat}$$<p>.</p>
<p>This is also the reason, why the LSTM architecture remedies the vanishing or exploding gradients. Instead of resulting in a huge product as in the backpropagation through time, we will simply add the gradients of each timestep's contribution [<a href="http://arunmallya.github.io/writeups/nn/lstm/index.html">Mallya 2017</a>].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we know what function we want to derive we also need to know which weights we want to tune. We define the weight matrices as</p>
\begin{alignat}{2}
% Weight matrices
\Wo &amp;= \begin{bmatrix} \wo{1} \\ \wo{2} \end{bmatrix} \quad \Uo &amp;&amp;= \begin{bmatrix} \uo{1}{1} &amp; \uo{1}{2} \\ \uo{2}{1} &amp; \uo{2}{2} \end{bmatrix} \\
\Wf &amp;= \begin{bmatrix} \wf{1} \\ \wf{2} \end{bmatrix} \quad \Uf &amp;&amp;= \begin{bmatrix} \uf{1}{1} &amp; \uf{1}{2} \\ \uf{2}{1} &amp; \uf{2}{2} \end{bmatrix} \\
\Wi &amp;= \begin{bmatrix} \wi{1} \\ \wi{2} \end{bmatrix} \quad \Ui &amp;&amp;= \begin{bmatrix} \ui{1}{1} &amp; \ui{1}{2} \\ \ui{2}{1} &amp; \ui{2}{2} \end{bmatrix} \\
\Wc &amp;= \begin{bmatrix} \wc{1} \\ \wc{2} \end{bmatrix} \quad \Uc &amp;&amp;= \begin{bmatrix} \uc{1}{1} &amp; \uc{1}{2} \\ \uc{2}{1} &amp; \uc{2}{2} \end{bmatrix}
\end{alignat}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, define the LSTM-forward pass as</p>
\begin{align}
% Forward pass
\myH[]{red} &amp;= \O * g(\C) \tag{1} \\
\C &amp;= \F * \C[-1] + \I * \Ctil \tag{2} \\
\Ctil &amp;= \Ctilfull\tag{3} \\
\I &amp;= \begin{pmatrix}
           \i{1} \\ \i{2}
      \end{pmatrix}
   = \Ifull \tag{4} \\
\F &amp;= \begin{pmatrix}
          \f{1} \\ \f{2}
      \end{pmatrix}
   = \Ffull  \tag{5} \\
\O &amp;= \begin{pmatrix}
          \o{1} \\ \o{2}
      \end{pmatrix}
   = \Ofull \tag{6}
\end{align}<p>Where we choose the activation functions $\sigma$ as the sigmoid function and $g$ as the hyperbolic function [<a href="https://arxiv.org/pdf/1701.05923.pdf">cp Dey, Salem 2017</a>].</p>
\begin{alignat}{3}
\sigma(x) &amp;= \frac{\exp(-x)}{1 + \exp(-x} &amp;&amp;\quad\text{with}\quad \deriv{\sigma(x)}{x} &amp;&amp;&amp;= \sigma(x) (1-\sigma(x)) \\
g(x) &amp;= \tanh(x) &amp;&amp;\quad\text{with}\quad \deriv{g(x)}{x} &amp;&amp;&amp;= 1 - \tanh^2(x)
\end{alignat}
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define activation functions</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sigmoid activation function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dsigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Derivative of sigmoid function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dtanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Derivative of tanh function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's also, for our purposes define the forward pass again, this time with a small change. This time, we want to store the results of every operation that depends on the timestep $t$ in their own lists. And we want to define a function that we can call later in a more convenient fashion.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Redefine forward pass</span>
<span class="n">h</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">o</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">i</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">c_</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">LSTM_forward</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Perform forward pass&quot;&quot;&quot;</span>

    <span class="c1"># First, set hidden state to zero</span>
    <span class="n">h</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wc</span><span class="p">))</span>
    <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wc</span><span class="p">))</span>

    <span class="c1"># Set `start` so that future calls don&#39;t have to reset the above lists</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y_</span><span class="p">)):</span>
        <span class="c1"># Calculate values of gates (equations 4, 5, 6)</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Wi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Ui</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
        <span class="n">ft</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Wf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
        <span class="n">ot</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Wo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>

        <span class="c1"># Calculate candidate update (equations 3 and 2)</span>
        <span class="n">c_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">Wc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
        <span class="n">ct</span> <span class="o">=</span> <span class="n">ft</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">it</span> <span class="o">*</span> <span class="n">c_t</span>

        <span class="c1"># Calculate hidden state (equation 1)</span>
        <span class="n">ht</span> <span class="o">=</span> <span class="n">ot</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>

        <span class="c1"># Prediction at step t (output layer)</span>
        <span class="n">y_t</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ht</span><span class="p">)</span>

        <span class="c1"># Save variables to container, these will</span>
        <span class="c1"># persist outside of the function&#39;s scope</span>
        <span class="n">h</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ht</span><span class="p">)</span>
        <span class="n">o</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ot</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ft</span><span class="p">)</span>
        <span class="n">i</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
        <span class="n">c_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
        <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
        <span class="n">y_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

    <span class="c1"># Return final prediction</span>
    <span class="k">return</span> <span class="n">y_</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">yhat_before</span> <span class="o">=</span> <span class="n">LSTM_forward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, in order to derive the loss function, we need to figure out how we obtain our predictions. We know that the output layer is just a linear combination of the hidden state and a vector of weights which, in our case, we denote by $w \in \mathbb{R}^2$.</p>
\begin{equation}
\yhat = w^T \cdot \myH[]{red}
\end{equation}<p>Of course, if you are familiar with matrix derivatives and know that the element-wise multiplication of two vectors can be written as</p>
\begin{equation}
a * b = \diag(a) \cdot b = \begin{bmatrix}
                               a_1 &amp; \dots &amp; 0 \\
                               \vdots &amp; \ddots &amp; \vdots \\
                               0 &amp; \dots &amp; a_n
                           \end{bmatrix} \cdot
                           \begin{pmatrix}
                                b_1 \\
                                \vdots \\
                                b_n
                           \end{pmatrix}
\end{equation}<p>then you're all set. For a derivation of all gradients using high-school-level math only, consider the appendix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-of-the-output-gate">Weights of the output gate<a class="anchor-link" href="#Weights-of-the-output-gate">&#182;</a></h3><p>By following the logic of the backpropagation we obtain:</p>
\begin{align}
\frac{\partial {loss}}{\partial W_o} &amp;= \sum\frac{\partial {loss}_t}{\partial \hat{y}_t} \overbrace{\frac{\partial \hat{y}_t}{\partial h_t} \frac{\partial h_t}{\partial o_t} \frac{\partial o_t}{\partial \Wo}}^{\deriv{\yhat}{\Wo}}\\
                                   &amp;= \sum -(y - \hat{y}_t)\ w * g(\C) * \Ofull[\prime] * \Ctil x_t.
\end{align}<p>Which we can implement in a straightforward manner.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wo</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Note that c = (c0, c1, c2, c3), whereas y_ = (y_1, y_2, y_3)</span>
    <span class="c1"># thus we need to index c differently</span>
    <span class="n">dLossdWo</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span><span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">c_</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">xt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similarly, we obtain the gradient with respect to $\Uo$:</p>
\begin{align}
\frac{\partial loss}{\partial U_o} &amp;= \sum\frac{\partial {loss}_t}{\partial \hat{y}_t} \overbrace{\frac{\partial \hat{y}_t}{\partial h_t} \frac{\partial h_t}{\partial o_t} \frac{\partial o_t}{\partial \Uo}}^{\deriv{\yhat}{\Uo}} \\
                                   &amp;= \sum -(y - \hat{y}_t)\ \begin{bmatrix} w &amp; w \end{bmatrix} *
                     \begin{bmatrix} g(\C) &amp; g(\C) \end{bmatrix} *
                     \begin{bmatrix} \Ofull[\prime] &amp; \Ofull[\prime] \end{bmatrix} *
                     \begin{bmatrix} \Ctil &amp; \Ctil \end{bmatrix} *
                     \begin{bmatrix} \myH{blue}^T \\ \myH{blue}^T \end{bmatrix}.
\end{align}<p>Note, that the first four of our matrix valued factors of that multiplication are obtained by combining copies of a vector in a column-wise fashion. In contrast to that, the last factor is obtained by row-wise stacking copies of a vector. In <code>numpy</code> one can obtain this behavior by a simple broadcasting operation. We simply need to <code>reshape</code> the first four factors, the expansion will then be performed automatically.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Uo</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dy_dot</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uo</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">c_</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="c1"># Expand so that the above multiplication can be performed element-wise</span>
    <span class="n">dLossdUo</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dy_dot</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-of-the-forget-gate">Weights of the forget gate<a class="anchor-link" href="#Weights-of-the-forget-gate">&#182;</a></h3><p>By the same token we obtain the partial gradient of $loss_t$ with respect to the elements of $\Wf$</p>
\begin{align}
\frac{\partial loss}{\partial W_f} &amp;=
    \sum\frac{\partial loss_t}{\partial \hat{y}_t}\overbrace{\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial c_t} \frac{\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial W_f}}^{\deriv{\yhat}{\Wf}} \\
                                   &amp;= \sum -(y-\hat{y}_t) w * \left[\O * g^\prime(\C)\right] * \C * \Ffull[\prime] \dxt{orange}
\end{align}<p>Which we, can implement as easily as before.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wf</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dLossdWf</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">xt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the gradient of $loss_t$ with respect to $\Uf$ we obtain:</p>
\begin{align}
\frac{\partial loss}{\partial U_f} &amp;= \sum\frac{\partial loss_t}{\partial \hat{y}_t} \overbrace{\frac{\partial \hat{y}_t}{\partial h_t} \frac{\partial h_t}{\partial c_t} \frac{\partial c_t}{\partial f_t} \frac{\partial f_t}{\partial U_f}}^{\deriv{\yhat}{\Uf}} \\
                                   &amp;= \sum -(y-\hat{y}_t)\ \begin{bmatrix} w &amp; w \end{bmatrix} *
                     \begin{bmatrix} \O &amp; \O \end{bmatrix} *
                     g^\prime( \begin{bmatrix} \C &amp; \C \end{bmatrix}) *
                     \begin{bmatrix} \Ffull[\prime] &amp; \Ffull[\prime] \end{bmatrix} *
                     \begin{bmatrix} \C[-1] &amp; \C[-1] \end{bmatrix} *
                     \begin{bmatrix} \myH{orange}^T \\ \myH{orange}^T \end{bmatrix}
\end{align}<p>Which we can implement, making use of <code>numpy</code>'s broadcasting operations, just as before.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Uf</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dy_dft</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">*</span><span class="n">dtanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span><span class="o">+</span><span class="n">Uf</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">dLossdUf</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dy_dft</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-of-the-input-gate">Weights of the input gate<a class="anchor-link" href="#Weights-of-the-input-gate">&#182;</a></h3><p>Looking at the weights of the input gate we can proceed in the familiar fashion. We procure the gradients of $loss_t$ with respect to $\Wi$ as</p>
\begin{align}
\frac{\partial loss}{\partial W_i} &amp;= \sum\frac{\partial loss_t}{\partial \hat{y}_t} \overbrace{\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial c_t} \frac{\partial c_t}{\partial i_t}\frac{\partial i_t}{\partial \Wi}}^{\deriv{\yhat}{\Wi}} \\
                                   &amp;= \sum -(y-\hat{y}_t)\ w * \O * g^\prime(\C) * \Ctil * \Ifull[\prime] \dxt{red}.
\end{align}<p>Subsequently, we can implement the computation of the gradient.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wi</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dLossdWi</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span> <span class="o">*</span> <span class="n">c_</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Ui</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">xt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Equivalently we proceed with the gradient of $loss_t$ with respect to $\Ui$, which are given by</p>
\begin{align}
\frac{\partial loss}{\partial U_i} &amp;= \sum\frac{\partial loss_t}{\partial \hat{y}_t} \overbrace{\frac{\partial \hat{y}_t}{\partial h_t} \frac{\partial h_t}{\partial c_t} \frac{\partial c_t}{\partial i_t} \frac{\partial i_t}{\partial U_i}}^{\deriv{\yhat}{\Ui}} \\
                                   &amp;= \sum(y-\hat{y}_t)\ \begin{bmatrix} w &amp; w \end{bmatrix} *
                      \begin{bmatrix} \O &amp; \O \end{bmatrix} *
                      g^\prime\left(\begin{bmatrix} \C &amp; \C \end{bmatrix}\right) *
                      \begin{bmatrix} \Ctil &amp; \Ctil \end{bmatrix} * \\
                      &amp;\quad\quad\begin{bmatrix} \Ifull[\prime] &amp; \Ifull[\prime] \end{bmatrix} *
                      \begin{bmatrix} \myH{red}^T \\ \myH{red}^T \end{bmatrix}
\end{align}<p>Which we implement by making use of <code>numpy</code>'s broadcasting functionality.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Ui</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dy_dit</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">*</span><span class="n">dtanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">c_</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span><span class="o">+</span><span class="n">Ui</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">dLossdUi</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dy_dit</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-of-the-candidate-update">Weights of the candidate update<a class="anchor-link" href="#Weights-of-the-candidate-update">&#182;</a></h3><p>Again we begin by computint the gradient of $loss_t$ with respect to $\Wc$, such that we get</p>
\begin{align}
\frac{\partial loss}{\partial W_c} &amp;= \sum\frac{\partial loss_t}{\partial \hat{y}_t}\overbrace{\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial \tilde{c}_t} \frac{\partial \tilde{c}_t}{\partial W_c}}^{\deriv{\yhat}{\Wc}} \\
                                   &amp;= \sum(y - \hat{y}_t)\ w * \O * g^\prime(\C) * \I * \Ctilfull[\prime]\ \dxt{green}
\end{align}<p>which we implement in the same unequivocal manner as before.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wc</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dLossdWc</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span> <span class="o">*</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">Wc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">xt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similarly, we obtain the gradient of the loss function with respect to $\Uc$ as</p>
\begin{align}
\frac{\partial loss}{\partial U_c} &amp;= \sum\frac{\partial loss_t}{\partial \hat{y}_t}\overbrace{\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial \tilde{c}_t}\frac{\partial \tilde{c}_t}{\partial U_c}}^{\deriv{\yhat}{\Uc}} \\
                          &amp;= \sum(y_t - \hat{y}_t)\ w * \O * g^\prime(\C) * \I * \Ctilfull[\prime]\ * \begin{bmatrix} h^T_{t-1} \\ h^T_{t-1} \end{bmatrix},
\end{align}<p>which by now should be easy to implement.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Uc</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dc_tdf</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span> <span class="o">*</span> <span class="n">i</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">Wc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span><span class="o">+</span><span class="n">Uc</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">dLossdUc</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dc_tdf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-of-the-output-layer">Weights of the output layer<a class="anchor-link" href="#Weights-of-the-output-layer">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we need to adjust the weights of the output layer. The gradient of the loss function is simply</p>
\begin{equation}
\deriv{loss}{w} = \sum\deriv{loss_t}{w} = \sum\deriv{loss_t}{\yhat}\deriv{\yhat}{w} = \sum -(y - \yhat) \myH[]{red}.
\end{equation}<p>Which can be readily implemented.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dLossdw</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="LSTM:-Update-the-weights">LSTM: Update the weights<a class="anchor-link" href="#LSTM:-Update-the-weights">&#182;</a></h3><p>With all gradients being computed, we can use them in order to update our weights. Typically, when using gradient descent, we follow the direction of the negative gradient, multiplied by some fraction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">Wc</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWc</span>
<span class="n">Wo</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWo</span>
<span class="n">Wf</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWf</span>
<span class="n">Wi</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWi</span>

<span class="n">Uc</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUc</span>
<span class="n">Uo</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUo</span>
<span class="n">Uf</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUf</span>
<span class="n">Ui</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUi</span>

<span class="n">w</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdw</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With the new weights we can compute the updated prediction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yhat_after</span> <span class="o">=</span> <span class="n">LSTM_forward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2">_</span><span class="si">{before}</span><span class="s2">$&quot;</span><span class="p">:</span> <span class="n">yhat_before</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2">_</span><span class="si">{after}</span><span class="s2">$&quot;</span><span class="p">:</span> <span class="n">yhat_after</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[16]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>$\hat{y}_{before}$</th>
      <th>$\hat{y}_{after}$</th>
      <th>$y$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>2.046038</td>
      <td>4.956741</td>
      <td>7.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can clearly see, we are getting close to the real $y$. In a real application one would of course evaluate the gradients at multiple samples and use an average gradient in order to update the weight matrices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Backpropagation-GRU">Backpropagation GRU<a class="anchor-link" href="#Backpropagation-GRU">&#182;</a></h3><p>$
% Vector shortcuts
\newcommand{\Htil}{\color{green}{\tilde{h}_t}}
\newcommand{\H}{\color{green}{h_{t-1}}}
\newcommand{\R}{\color{blue}{r_t}}
\newcommand{\Z}{\color{red}{z_t}}
% Gates Matrix Notation
\newcommand{\Gfull}[1][]{\color{green}{g^{#1}\left( W_h x_t + U_h \cdot ( \R * \H)\right)}}
\newcommand{\Zfull}[1][]{\color{red}{\sigma^{#1} \left( W_z x_t + U_z \cdot h_{t-1} \right)}}
\newcommand{\Rfull}[1][]{\color{blue}{\sigma^{#1} \left( W_r x_t + U_r \cdot h_{t-1} \right)}}
% Vector Elements
\newcommand{\htil}[1]{\color{green}{\tilde{h}^t_{#1}}}
\newcommand{\h}[1]{\color{green}{h_{#1}^{t-1}}}
\newcommand{\z}[1]{\color{red}{z_{#1}^{t}}}
\newcommand{\r}[1]{\color{blue}{r_{#1}^{t}}}
% Explicit Vector Elements
\newcommand{\gfull}[2][]{\color{green}{g^{#1} \left( w^h_{#2} x_t + u^h_{#2 1} \r{1} \h{1} + u^h_{#2 2} \r{2} \h{2} \right)}}
\newcommand{\zfull}[2][]{\color{red}{\sigma^{#1} \left(w^z_{#2} x_t + u^z_{#2 1}h_1^{t-1}+u^z_{#2 2} h_2^{t-1}\right)}}
\newcommand{\rfull}[2][]{\color{blue}{\sigma^{#1} \left(w^r_{#2} x_t + u^r_{#2 1}h_1^{t-1}+u^r_{#2 2} h_2^{t-1}\right)}}
\newcommand{\gfullfull}[2][]{\color{green}{g^{#1} \left( w^h_{#2} x_t + u^h_{#2 1} \rfull{1} \h{1} + u^h_{#2 2} \rfull{2} \h{2} \right)}}
% Weight Matrices
\newcommand{\Wh}{\color{green}{W_h}}
\newcommand{\Wz}{\color{red}{W_z}}
\newcommand{\Wr}{\color{blue}{W_r}}
\newcommand{\Uh}{\color{green}{U_h}}
\newcommand{\Uz}{\color{red}{U_z}}
\newcommand{\Ur}{\color{blue}{U_r}}
% Weight Matrix Elements
\newcommand{\wh}[1]{\color{green}{w^h_{#1}}}
\newcommand{\wz}[1]{\color{red}{w^z_{#1}}}
\newcommand{\wr}[1]{\color{blue}{w^r_{#1}}}
\newcommand{\uh}[2]{\color{green}{u^h_{#1 #2}}}
\newcommand{\uz}[2]{\color{red}{u^z_{#1 #2}}}
\newcommand{\ur}[2]{\color{blue}{u^r_{#1 #2}}}
% Miscellaneous
\newcommand{\dxt}[1]{\color{#1}{x_t}}
\newcommand{\yhat}{\hat{y}_t}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\diag}{diag}
$
Consider again, our low-dimensional numerical example with only one case with features $x = \begin{pmatrix}x_1, &amp; x_2, &amp; x_3 \end{pmatrix}^T$ and label $y$. Additionally let's again assume we only have two neurons in our network. This means for the weight matrices</p>
\begin{alignat}{2}
\color{green}{W_h} &amp;= \begin{bmatrix} \wh{1} \\ \wh{2} \end{bmatrix}  \quad
\color{green}{U_h} &amp;&amp;= \begin{bmatrix} \uh{1}{1} &amp; \uh{1}{2} \\ \uh{2}{1} &amp; \uh{2}{2}\end{bmatrix}\\
\color{red}{W_z} &amp;= \begin{bmatrix} \wz{1} \\ \wz{2} \end{bmatrix}  \quad
\color{red}{U_z} &amp;&amp;= \begin{bmatrix} \uz{1}{1} &amp; \uz{1}{2} \\ \uz{2}{1} &amp; \uz{2}{2} \end{bmatrix}\\
\color{blue}{W_r} &amp;= \begin{bmatrix} \wr{1} \\ \wr{2} \end{bmatrix} \quad
\color{blue}{U_r} &amp;&amp;= \begin{bmatrix} \ur{1}{1} &amp; \ur{1}{2} \\ \ur{2}{1} &amp; \ur{2}{2} \end{bmatrix}
\end{alignat}<p>We define the potential candidate update as
\begin{equation}
\Htil = \Gfull,
\end{equation}</p>
<p>the gates as</p>
\begin{align}
\Z &amp;= \Zfull \quad\text{and}\\
\R &amp;= \Rfull,
\end{align}<p>where as before, $g$ is the $tanh$-function and $\sigma$ the sigmoid function.</p>
<p>Finally, we define the update of the hidden state as
\begin{equation}
h_t = \color{red}{1-\Z} * h_{t-1} + \Z * \Htil
\end{equation} [<a href="https://arxiv.org/pdf/1701.05923.pdf">cp Rey, Salem, 2017</a>].</p>
<p>For the actual prediction at a step $t$, we connect every output of our hidden state in a dense layer. Which means that we are taking a weighted sum of all two of them.</p>
\begin{equation}
\hat{y}_t = W^T h_t,
\end{equation}<p>Where $W = \begin{pmatrix} w_1 \\ w_2 \end{pmatrix}$ is the matrix that contains the weights for the outer layer. In our case this is just a $2\times 1$ vector.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the same simple example containing just one sample $x \in \mathbb{R}^2$, from before.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can implement the forward pass like that:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">h_</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y_</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">GRU_forward</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Perform forward pass.&quot;&quot;&quot;</span>
    <span class="n">h</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wh</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y_</span><span class="p">)):</span>
        <span class="c1"># Calculate values of the gates</span>
        <span class="n">zt</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Wz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
        <span class="n">rt</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Wr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Ur</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>

        <span class="c1"># Calculate candidate update</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rt</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>

        <span class="c1"># Calculate hidden state</span>
        <span class="n">ht</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">zt</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">zt</span> <span class="o">*</span> <span class="n">h_t</span>

        <span class="c1"># Calculate prediction at step t</span>
        <span class="n">y_t</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ht</span><span class="p">)</span>

        <span class="c1"># Save variables to container</span>
        <span class="n">h</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ht</span><span class="p">)</span>
        <span class="n">h_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">zt</span><span class="p">)</span>
        <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rt</span><span class="p">)</span>
        <span class="n">y_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">y_</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">yhat_before</span> <span class="o">=</span> <span class="n">GRU_forward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Where we want to keep track of the prediction before updating the weights, such that we can compare it with the prediction we obtain after we update our weight matrices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-for-the-candidate-$\tilde{h}_t$">Weights for the candidate $\tilde{h}_t$<a class="anchor-link" href="#Weights-for-the-candidate-$\tilde{h}_t$">&#182;</a></h3><p>Let's start with investigating the gradient of $loss_t$ with respect to $\Wh$:</p>
\begin{align}
\frac{\partial {loss}_t}{\partial W_h} &amp;= \frac{\partial {loss}_t}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial \tilde{h}_t}\frac{\partial \tilde{h}_t}{\partial \Wh} \\
                                   &amp;= -(y-\hat{y}_t) W * \Z * \Gfull[\prime] \color{green}{x_t}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wh</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Note that `h` has an entry at start, so indexing at t accesses h_{t-1}</span>
    <span class="n">dh_tdWh</span> <span class="o">=</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">xt</span>
    <span class="n">dLossdWh</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dh_tdWh</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, following the logic of the backpropagation algorithm for the gradient of $loss_t$ with respect to $\Uh$, we obtain</p>
\begin{align}
\frac{\partial {loss}_t}{\partial \Uh} &amp;= \frac{\partial {loss}_t}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial \tilde{h}_t}\frac{\partial \tilde{h}_t}{\partial \Uh} \\
                                    &amp;= -(y-\hat{y}_t)\ \diag \left[ w * \Z * \Gfull[\prime]\right] \cdot  \begin{bmatrix} (\R * \H)^T \\ (\R * \H)^T \end{bmatrix}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Uh</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dy_dh_t</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">dLossdUh</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dy_dh_t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-for-the-update-gate-$z_t$">Weights for the update gate $z_t$<a class="anchor-link" href="#Weights-for-the-update-gate-$z_t$">&#182;</a></h3><p>Using the same logic, we calculate the gradient of $loss_t$ with respect to $\Wz$, which gives</p>
\begin{align}
\frac{\partial {loss}_t}{\partial W_z} &amp;= \frac{\partial {loss}_t}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial z_t}\frac{\partial z_t}{\partial W_z} \\
                                   &amp;= -(y-\hat{y}_t)\ w * \begin{bmatrix} -\H + \Htil \end{bmatrix} * \Zfull[\prime] \dxt{red}.
\end{align}<p>Which in turn we can translate into <code>Python</code> code.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wz</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dztdWz</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">*</span> <span class="n">xt</span>
    <span class="n">dLossdWz</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">h_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dztdWz</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By now this should be getting old. We use the previous derivations in order to find the gradient of the loss function with respect to $\Uz$, where now one can easily see that:</p>
\begin{align}
\frac{\partial {loss}_t}{\partial \Uz} &amp;= \frac{\partial loss}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial z_t}\frac{\partial z_t}{\partial \Uz} \\
                                   &amp;= -(y-\hat{y}_t)\ \diag \left[ w * [-\H + \Htil] * \Zfull[\prime]\right] \cdot \begin{bmatrix} \H^T \\ \H^T \end{bmatrix}.
\end{align}<p>Having done that, we can implement it in the now too familiar way.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Uz</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">dy_dzt</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uz</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))]</span>
    <span class="n">dLossdUz</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dy_dzt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-for-the-reset-gate-$r_t$">Weights for the reset gate $r_t$<a class="anchor-link" href="#Weights-for-the-reset-gate-$r_t$">&#182;</a></h3><p>Similarly, we can proceed with obtaining the gradients of $loss_t$ with respect to the elements of the reset gate.</p>
\begin{align}
\frac{\partial {loss}_t}{\partial \Wr} &amp;= \frac{\partial {loss}_t}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial \tilde{h}_t}\frac{\partial \tilde{h}_t}{\partial \Wr} \\
                                   &amp;= -(y-\hat{y}_t)\ w^T \cdot \left(
\begin{bmatrix} \Z &amp; \Z \end{bmatrix} *
\begin{bmatrix} \Gfull[\prime] &amp; \Gfull[\prime] \end{bmatrix} *
\Uh *
\begin{bmatrix} \H^T \\ \H^T \end{bmatrix} *
\begin{bmatrix} \Rfull[\prime]^T \\ \Rfull[\prime]^T \end{bmatrix}
\right) \dxt{blue}.
\end{align}<p>Which we also implement in <code>Python</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdWr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wr</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z_dg</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">d_rt</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Ur</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">dLossdWr</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z_dg</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">Uh</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">d_rt</span><span class="p">)</span> <span class="o">*</span> <span class="n">xt</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we compute the gradient of $loss_t$ with respect to $\Ur$</p>
\begin{align}
\deriv{loss_t}{\Ur} &amp;= -(y-\yhat) \begin{bmatrix}(w * \Z * \Gfull[\prime])^T \\ (w * \Z * \Gfull[\prime])^T \end{bmatrix} \cdot \Uh^T  *
\begin{bmatrix} \H \cdot \H^T \end{bmatrix} *
\begin{bmatrix} \Rfull[\prime] &amp; \Rfull[\prime] \end{bmatrix}.
\end{align}<p>We implement this gradient in the following way:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dLossdUr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Ur</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">xt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">w_z_dg</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">z</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">dtanh</span><span class="p">(</span><span class="n">Wh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="o">+</span> <span class="n">Uh</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="n">outer_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">dr</span> <span class="o">=</span> <span class="n">dsigmoid</span><span class="p">(</span><span class="n">Wr</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span><span class="o">+</span><span class="n">Ur</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">dLossdUr</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">w_z_dg</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Uh</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">outer_h</span> <span class="o">*</span> <span class="n">dr</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="GRU:-Update-the-weights">GRU: Update the weights<a class="anchor-link" href="#GRU:-Update-the-weights">&#182;</a></h3><p>Now that we have computed the gradients of the loss function with respect to any of the weight matrices, we can update the weights and then see if this did indeed improve the prediction of our toy example.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">Wr</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWr</span>
<span class="n">Wz</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWz</span>
<span class="n">Wh</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdWh</span>

<span class="n">Ur</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUr</span>
<span class="n">Uz</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUz</span>
<span class="n">Uh</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">dLossdUh</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we perform the forward pass again with the updated weights.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yhat_after</span> <span class="o">=</span> <span class="n">GRU_forward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2">_</span><span class="si">{before}</span><span class="s2">$&quot;</span><span class="p">:</span> <span class="n">yhat_before</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2">_</span><span class="si">{after}</span><span class="s2">$&quot;</span><span class="p">:</span> <span class="n">yhat_after</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[29]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>$\hat{y}_{before}$</th>
      <th>$\hat{y}_{after}$</th>
      <th>$y$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>5.14414</td>
      <td>5.986334</td>
      <td>7.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And indeed, we are better than before. If we were to reiterate these steps multiple times we could get arbitrarily close to the true value of $7$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">SimpleRNN</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span>
<span class="kn">from</span> <span class="nn">models.recurrent</span> <span class="k">import</span> <span class="n">Recurrent</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="k">import</span> <span class="n">tqdm_notebook</span> <span class="k">as</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span> <span class="nn">hyperopt</span> <span class="k">import</span> <span class="n">tpe</span><span class="p">,</span> <span class="n">hp</span><span class="p">,</span> <span class="n">fmin</span><span class="p">,</span> <span class="n">STATUS_OK</span><span class="p">,</span> <span class="n">Trials</span>
<span class="kn">from</span> <span class="nn">hyperopt.pyll.base</span> <span class="k">import</span> <span class="n">scope</span>

<span class="kn">from</span> <span class="nn">pprint</span> <span class="k">import</span> <span class="n">pprint</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Parameter-Tuning-using-hyperopt">Parameter Tuning using <code>hyperopt</code><a class="anchor-link" href="#Parameter-Tuning-using-hyperopt">&#182;</a></h2><p>Since we now know what we are doing and how our models work it's time to turn our attention back to our initial problem at hand and to fine tune our neural nets. Since we want to be able to make a fair comparison among the three different types of neural networks we need to tune them in a similar fashion, and to be precise we should also tune the same hyperparameters for them. A nice way to do that is to use the <code>hyperopt</code> module. With it, we will use a Tree of Parzen Estimators to find the optimal set of hyperparameters (for more information regarting Tree of Parzen Estimators consider the Appendix).</p>
<p>Additionally, to facilitate the fitting of numerous models we opted to automate the data generation process and to gather other assorted functionality in a custom built subclass of the <code>keras.Sequential</code> class. We will explain selected features of this handcrafted <code>models.Recurrent</code> class along the way. There is also a full <a href="https://github.com/thsis/INFOSYS/blob/master/README.md">documentation</a> available.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-the-series-by-district:">Load the series by district:<a class="anchor-link" href="#Load-the-series-by-district:">&#182;</a></h3><p>First we want to load the data and here we want to explicitly parse the dates inside the <code>Date</code> column and specify a <code>MultiIndex</code> where the levels are <code>Date</code> and then <code>District</code>. This allows our <code>models.Recurrent</code>-class to distinguish between the series for the whole of Chicago and the Chicago crime series by each district.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">datapath_district</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;crimes_district.csv&quot;</span><span class="p">)</span>
<span class="n">district</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">datapath_district</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="s2">&quot;District&quot;</span><span class="p">],</span>
                       <span class="n">dtype</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;District&quot;</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span>
                              <span class="s2">&quot;Incidents&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">},</span>
                       <span class="n">parse_dates</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Date&quot;</span><span class="p">])</span>
<span class="n">district</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[26]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Date</th>
      <th colspan="21" halign="left">2001-01-01</th>
    </tr>
    <tr>
      <th>District</th>
      <th>1.0</th>
      <th>2.0</th>
      <th>3.0</th>
      <th>4.0</th>
      <th>5.0</th>
      <th>6.0</th>
      <th>7.0</th>
      <th>8.0</th>
      <th>9.0</th>
      <th>10.0</th>
      <th>...</th>
      <th>16.0</th>
      <th>17.0</th>
      <th>18.0</th>
      <th>19.0</th>
      <th>20.0</th>
      <th>21.0</th>
      <th>22.0</th>
      <th>24.0</th>
      <th>25.0</th>
      <th>31.0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Incidents</th>
      <td>37.0</td>
      <td>110.0</td>
      <td>103.0</td>
      <td>96.0</td>
      <td>95.0</td>
      <td>84.0</td>
      <td>83.0</td>
      <td>111.0</td>
      <td>109.0</td>
      <td>104.0</td>
      <td>...</td>
      <td>67.0</td>
      <td>67.0</td>
      <td>72.0</td>
      <td>72.0</td>
      <td>40.0</td>
      <td>0.0</td>
      <td>61.0</td>
      <td>59.0</td>
      <td>120.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 24 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, this is just a series and not exactly a matrix of features, as we know it from traditional statistics or even other architectures for neural networks. Therefore we take the series as it is to be our <strong>target</strong> variable and we construct multiple columns by lagging the series by up to $l$ days, where $l \in \mathbb{N}$. If the maximum number of lags is, for example, $l = 3$ this means that we try to predict the number of cases today, just by knowing the number of cases that occured the previous day, the day before that and the day before that day. This is handily included in the <code>models.Recurrent</code> class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Recurrent</span><span class="p">(</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="c1"># Lots of code...</span>

    <span class="k">def</span> <span class="nf">__get_features</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxlag</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">colname</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">MultiIndex</span><span class="p">):</span>
                <span class="n">lag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cross_label</span><span class="p">)[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">colname</span><span class="p">]</span> <span class="o">=</span> <span class="n">lag</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Even more code...</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We include the maximum number of lags as another hyperparameter, in order to cut down on training time. Besides, we have no reason to believe that the number of crime incidents in the far past, say more than a year ago, would be a good indicator for the number of occuring acts of crime tomorrow.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Separate-data-and-a-holdout-set.">Separate data and a holdout set.<a class="anchor-link" href="#Separate-data-and-a-holdout-set.">&#182;</a></h3><p>Because we use a library that tunes parameters with regards to the test set, we need to separate a holdout set. On this set, which we will not expose to our models we can evaluate the performance of our tuned models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define breaking points for train and holdout set.</span>
<span class="n">lower</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2015-01-01&quot;</span><span class="p">)</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2017-01-01&quot;</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s2">&quot;2018-01-01&quot;</span><span class="p">)</span>
<span class="c1"># Divide dataset.</span>
<span class="n">district_data</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">district</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">district</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lower</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">upper</span><span class="p">),</span> <span class="p">:])</span>
<span class="n">district_holdout</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">district</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">district</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">upper</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">),</span> <span class="p">:])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Hyperopt-parameter-spaces">Hyperopt parameter spaces<a class="anchor-link" href="#Hyperopt-parameter-spaces">&#182;</a></h3><p>Most of our models have similar parameters which we want to tune. For the sampling of the subspace we use mostly uniform distributions, since they impose the least a-priori assumptions on where the optimal parameters lie. Additionally, mostly due to hardware limitations, we restrict the number of maximum lags to be no higher than $100$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># add optimizer, learn-rate, </span>
<span class="n">paramspace</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;maxlag&quot;</span><span class="p">:</span> <span class="n">scope</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s2">&quot;maxlag&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
              <span class="s2">&quot;cell_neurons&quot;</span><span class="p">:</span> <span class="n">scope</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s2">&quot;cell_neurons&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
              <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">scope</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
              <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="s2">&quot;sgd&quot;</span><span class="p">])}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have defined a base parameter space dictionary which we update with the parameters which are more specific to each cell. This can get out of hand pretty fast, therefore we generate a dictionary through a loop:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">spacesdict</span> <span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">cell</span> <span class="ow">in</span> <span class="p">(</span><span class="n">SimpleRNN</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">GRU</span><span class="p">):</span>
    <span class="n">spacesdict</span><span class="p">[</span><span class="n">cell</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cell&quot;</span><span class="p">:</span> <span class="n">cell</span><span class="p">,</span> <span class="o">**</span><span class="n">paramspace</span><span class="p">}</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">spacesdict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;GRU&#39;: {&#39;batch_size&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2f9b0&gt;,
         &#39;cell&#39;: &lt;class &#39;keras.layers.recurrent.GRU&#39;&gt;,
         &#39;cell_neurons&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2f7f0&gt;,
         &#39;maxlag&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b28e10&gt;,
         &#39;optimizer&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2fa90&gt;},
 &#39;LSTM&#39;: {&#39;batch_size&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2f9b0&gt;,
          &#39;cell&#39;: &lt;class &#39;keras.layers.recurrent.LSTM&#39;&gt;,
          &#39;cell_neurons&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2f7f0&gt;,
          &#39;maxlag&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b28e10&gt;,
          &#39;optimizer&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2fa90&gt;},
 &#39;SimpleRNN&#39;: {&#39;batch_size&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2f9b0&gt;,
               &#39;cell&#39;: &lt;class &#39;keras.layers.recurrent.SimpleRNN&#39;&gt;,
               &#39;cell_neurons&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2f7f0&gt;,
               &#39;maxlag&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b28e10&gt;,
               &#39;optimizer&#39;: &lt;hyperopt.pyll.base.Apply object at 0x7fae81b2fa90&gt;}}
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to store the results of each <code>hyperopt</code>-search we can create a <code>hyperopt.Trials</code>-object, which is very similar to a native Python <code>dictionary</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trialsdict</span> <span class="o">=</span> <span class="p">{</span><span class="n">model</span><span class="p">:</span> <span class="n">Trials</span><span class="p">()</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;SimpleRNN&quot;</span><span class="p">,</span> <span class="s2">&quot;LSTM&quot;</span><span class="p">,</span> <span class="s2">&quot;GRU&quot;</span><span class="p">)}</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">trialsdict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;GRU&#39;: &lt;hyperopt.base.Trials object at 0x7fae9e4e7978&gt;,
 &#39;LSTM&#39;: &lt;hyperopt.base.Trials object at 0x7fae9e4e77b8&gt;,
 &#39;SimpleRNN&#39;: &lt;hyperopt.base.Trials object at 0x7fae9e4e7908&gt;}
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Optimizing-over-the-parameter-space">Optimizing over the parameter space<a class="anchor-link" href="#Optimizing-over-the-parameter-space">&#182;</a></h3><p>The design of the <code>hyperopt</code>-module demands of us to define an objective function, which we then optimize by calling the <code>hyperopt.fmin</code> function. And of course, we could do that. However, the objective function will be extremely costly to evaluate - and there is nothing to be done about that - we only add a progress bar, since there is no visual representation of the algorithms' progress. That's why we will use the <code>Python</code>-decorator syntax, where we decorate our model call with the progress bar from the <code>tqdm</code> module and the <code>fmin</code>-function from <code>hyperopt</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">minimizer</span><span class="p">(</span><span class="n">objective</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">outer</span><span class="p">(</span><span class="n">paramspace</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">max_evals</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generate an inner objective-function and optimize it.&quot;&quot;&quot;</span>
        <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">max_evals</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="n">paramspace</span><span class="p">[</span><span class="s2">&quot;cell&quot;</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Update the progress bar and call the objective function.&quot;&quot;&quot;</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">objective</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">best</span> <span class="o">=</span> <span class="n">fmin</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span>
                    <span class="n">space</span><span class="o">=</span><span class="n">paramspace</span><span class="p">,</span>
                    <span class="n">algo</span><span class="o">=</span><span class="n">tpe</span><span class="o">.</span><span class="n">suggest</span><span class="p">,</span>
                    <span class="n">max_evals</span><span class="o">=</span><span class="n">max_evals</span><span class="p">,</span>
                    <span class="n">trials</span><span class="o">=</span><span class="n">trials</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">best</span>
    <span class="k">return</span> <span class="n">outer</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This way, each time the nested function <code>inner</code> gets called, it will update the progress bar. Giving us a rough idea on how long it will take until we get our results (unfortunately it does not seem to render correctly in HTML).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now to the function we wish to decorate. It contains 3 steps:</p>
<ol>
<li>Create a model with a certain set of (yet undefined) parameters.</li>
<li>Train the model.</li>
<li>Calculate the loss on a test set and return a dictionary, that contains the loss and a flag indicating that everything went okay.</li>
</ol>
<p>We choose a very simplistic architecture, where we have only the input layer, a hidden layer which we call - in accordance to the <code>keras</code> documentation - a <em>cell</em>. This <em>cell</em> can be a <code>keras.SimpleRNN</code>, a <code>keras.LSTM</code> or a <code>keras.GRU</code> object. Finally, we add a densely connected output layer, that we provide with a Rectified Linear Unit (ReLu) activation function, this step is just a precaution, since we do not want our model to generate predictions which are lower than zero.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Recurrent</span><span class="p">(</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="c1"># Lots of code...</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train the model based on parameters passed to `__init__`.&quot;&quot;&quot;</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__transform_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_neurons</span><span class="p">,</span>
                           <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxlag</span><span class="p">),</span>
                           <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">cellkwargs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lossfunc</span><span class="p">,</span>
                     <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
                     <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span>
                 <span class="n">epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                 <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">fitkwargs</span><span class="p">)</span>

    <span class="c1"># Further code...</span>
</pre></div>
<p>And while defining the function, we can pass it directly to our decorator.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@minimizer</span>
<span class="k">def</span> <span class="nf">district_get_loss</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return loss on test set.&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Recurrent</span><span class="p">(</span><span class="n">district_data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">predictions</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;status&#39;</span><span class="p">:</span> <span class="n">STATUS_OK</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above syntax is just a shortcut for</p>
<div class="highlight"><pre><span></span><span class="n">district_get_loss</span> <span class="o">=</span> <span class="n">minimizer</span><span class="p">(</span><span class="n">district_get_loss</span><span class="p">)</span>
</pre></div>
<p>And now to tuning the hyperparameters. Again we pass through all our models and store the best hyperparameters in a dictionary.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="s2">&quot;SimpleRNN&quot;</span><span class="p">,</span> <span class="s2">&quot;LSTM&quot;</span><span class="p">,</span> <span class="s2">&quot;GRU&quot;</span><span class="p">:</span>
    <span class="n">best</span><span class="p">[</span><span class="n">model</span><span class="p">]</span> <span class="o">=</span> <span class="n">district_get_loss</span><span class="p">(</span><span class="n">spacesdict</span><span class="p">[</span><span class="n">model</span><span class="p">],</span>
                                    <span class="n">trialsdict</span><span class="p">[</span><span class="n">model</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>







<div id="89c6b5e2-440d-4b12-bd5e-d5ac2d560efe"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#89c6b5e2-440d-4b12-bd5e-d5ac2d560efe');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "855ba9167bb64727bea92e98c8159619", "version_minor": 0, "version_major": 2}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>







<div id="e85be4cd-ad24-4b95-8f7b-56e5aa4016ef"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#e85be4cd-ad24-4b95-8f7b-56e5aa4016ef');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "5eae0e52b965416795ca91eb2c0c9f85", "version_minor": 0, "version_major": 2}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>







<div id="d2a6bea5-e770-4b7c-8a4e-222a6957e203"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#d2a6bea5-e770-4b7c-8a4e-222a6957e203');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "c4aaccebcf7e4d56bccbc2ae6954cc11", "version_minor": 0, "version_major": 2}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The reason why we use dictionaries so much is, because we can easily save them as a <code>json</code>-file.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">,</span> <span class="s1">&#39;analysis&#39;</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">,</span> <span class="s1">&#39;best_params.json&#39;</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">outfile</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">best</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;GRU&#39;: {&#39;batch_size&#39;: 2.0,
         &#39;cell_neurons&#39;: 26.0,
         &#39;maxlag&#39;: 44.0,
         &#39;optimizer&#39;: 1},
 &#39;LSTM&#39;: {&#39;batch_size&#39;: 5.0,
          &#39;cell_neurons&#39;: 17.0,
          &#39;maxlag&#39;: 84.0,
          &#39;optimizer&#39;: 1},
 &#39;SimpleRNN&#39;: {&#39;batch_size&#39;: 39.0,
               &#39;cell_neurons&#39;: 24.0,
               &#39;maxlag&#39;: 22.0,
               &#39;optimizer&#39;: 1}}
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We see that the optimal hyperparameters for our different models do not differ too much. Especially the number of neurons stays roughly the same accross architectures. However, that does <strong>not</strong> mean that the number of weights is similar, by construction the <em>LSTM</em> has the most weights, followed by the <em>GRU</em>, the <em>RNN</em> has the least amount of weights that need to be trained. The most remarkable difference lies in the batch size, where both <em>LSTM</em> and <em>GRU</em> needed a smaller batch size to be trained effectively, which subsequently increases the time it needs to train them. Also it seams that the LSTM made use of much longer sequences than the GRU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluate-on-Holdout-set">Evaluate on Holdout set<a class="anchor-link" href="#Evaluate-on-Holdout-set">&#182;</a></h3><p>In order to properly evaluate our models we retrain them, using the optimal parameters as well as the data which until now has been left untouched. We can easily do that from within our custom <code>models.Recurrent</code> class. We just need to find the size of our new train set as a percentage of the original dataset.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">district_data</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">district_data</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">district_holdout</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">district</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">district</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lower</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we can train the final models and calculate the mean absolute and the mean squared error on the holdout set.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">typecast</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fix types of hyperopt.Trials&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="s2">&quot;sgd&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span>
            <span class="c1"># val is 0 or 1, transform into string</span>
            <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizers</span><span class="p">[</span><span class="n">val</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># take a float, turn it into an int</span>
                <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
                <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">name2cell</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;RNN&quot;</span><span class="p">:</span> <span class="n">SimpleRNN</span><span class="p">,</span>
             <span class="s2">&quot;LSTM&quot;</span><span class="p">:</span> <span class="n">LSTM</span><span class="p">,</span>
             <span class="s2">&quot;GRU&quot;</span><span class="p">:</span> <span class="n">GRU</span><span class="p">}</span>

<span class="n">series_col</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cell_col</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_L1_col</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_L2_col</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">name2cell</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">bestparams</span> <span class="o">=</span> <span class="n">typecast</span><span class="p">({</span><span class="s2">&quot;cell&quot;</span><span class="p">:</span> <span class="n">cell</span><span class="p">,</span>
                          <span class="o">**</span><span class="n">best</span><span class="p">[</span><span class="n">cell</span><span class="o">.</span><span class="vm">__name__</span><span class="p">]})</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Recurrent</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">train_size</span><span class="p">,</span>
                      <span class="o">**</span><span class="n">bestparams</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">loss_L1</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span>
                                  <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">loss_L2</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span>
                                 <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="n">cell_col</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">loss_L2_col</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_L2</span><span class="p">)</span>
    <span class="n">loss_L1_col</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_L1</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>







<div id="6ab8793f-d696-46e5-bd74-d4c8a161e70c"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#6ab8793f-d696-46e5-bd74-d4c8a161e70c');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "ccc7c14d541c4615a0c96992dfce1ec2", "version_minor": 0, "version_major": 2}
</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, we compare the models to a naive model, that simply outputs the previous days' number of reported crimes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Naive Model: prediction equal to previous day.</span>
<span class="n">district_holdout</span><span class="p">[</span><span class="s2">&quot;Baseline&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">district_holdout</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;District&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">district_holdout</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calculate baseline model performance.</span>
<span class="n">cell_col</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Baseline&quot;</span><span class="p">)</span>
<span class="n">baseline_L1</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">district_holdout</span><span class="o">.</span><span class="n">Incidents</span><span class="p">,</span>
                                  <span class="n">y_pred</span><span class="o">=</span><span class="n">district_holdout</span><span class="o">.</span><span class="n">Baseline</span><span class="p">)</span>
<span class="n">loss_L1_col</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">baseline_L1</span><span class="p">)</span>
<span class="n">baseline_L2</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">district_holdout</span><span class="o">.</span><span class="n">Incidents</span><span class="p">,</span>
                                 <span class="n">y_pred</span><span class="o">=</span><span class="n">district_holdout</span><span class="o">.</span><span class="n">Baseline</span><span class="p">)</span>
<span class="n">loss_L2_col</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">baseline_L2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we save everything to a <code>pd.DataFrame</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">validation</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Model&quot;</span><span class="p">:</span> <span class="n">cell_col</span><span class="p">,</span>
                           <span class="s2">&quot;Validation L1 Loss&quot;</span><span class="p">:</span> <span class="n">loss_L1_col</span><span class="p">,</span>
                           <span class="s2">&quot;Validation L2 Loss&quot;</span><span class="p">:</span> <span class="n">loss_L2_col</span><span class="p">})</span>
<span class="n">validation</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">,</span> <span class="s2">&quot;analysis&quot;</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">,</span> <span class="s2">&quot;loss.csv&quot;</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Results-and-Comparison-of-the-Models">Results and Comparison of the Models<a class="anchor-link" href="#Results-and-Comparison-of-the-Models">&#182;</a></h3><p>After testing all our models, we get to the results represented in the below table. We evaluate our models by computing the Mean Absolute Error (L1 loss) and Mean Squared Error (L2 loss). As a baseline model, we use a model that only predicts the last day, i.e. the number of crimes of today are equal to the number of crimes of yesterday.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table>
<thead><tr>
<th>Model</th>
<th>Validation L1 Loss</th>
<th>Validation L2 Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>6.57</td>
<td>78.39</td>
</tr>
<tr>
<td>RNN</td>
<td>5.13</td>
<td>48.67</td>
</tr>
<tr>
<td>LSTM</td>
<td>4.97</td>
<td>46.15</td>
</tr>
<tr>
<td>GRU</td>
<td>4.96</td>
<td>45.57</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All our models perform around 23 percent better than the baseline model when looking at the L1 Loss, i.e. the loss by absolute number of crime incidences. The models perform similar whereas the results of the simple RNN are not as good as of the LSTM and the GRU model. This might be due to the vanishing gradient problem that can be prevented with LSTM and GRU. Furthermore, these two have very similar loss values.</p>
<p>In general, the GRU can be seen as less complex and faster when having lots of data. The LSTM and GRU perform similar, but usually both models are tested to see what performs better for the given data. The LSTM often times reaches better performance than the GRU. This can be argued to be due to the additional cell state which can hold on to information that is not needed for the output and thus can remember information better regarding long-term dependencies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Appendix">Appendix<a class="anchor-link" href="#Appendix">&#182;</a></h2><h3 id="Tree-of-Parzen-Estimators">Tree of Parzen Estimators<a class="anchor-link" href="#Tree-of-Parzen-Estimators">&#182;</a></h3><h4 id="General-Idea">General Idea<a class="anchor-link" href="#General-Idea">&#182;</a></h4><p>Typically, researchers are confronted with the problem that it takes considerable amounts of time when trying to evaluate a cost function. This problem is especially aggravating, when the optimization of that costly objective function is performed over a large grid of hyperparameters.</p>
<p>The general idea of the tree of Parzen Estimators is remarkably simple. When we evaluate the objective function on multiple sets of hyperparameters $x$, we can always fix a threshold $y^\star$ such that some evaluations are below that threshold, and some lie above. Effectively this means, that we have split our different evaluations into two parts: a good one, where the evaluated objective function lies below $y^\star$ and a second part that leads to a worse loss larger than $y^\star$, which is bad in our case.</p>
<p>Mathematically speaking, we can split the conditional densities of our hyperparameters $x$ into two parts, such that</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{equation}
p(x|y) = \begin{cases}
\color{blue}{l(x)} \text{ if } y&lt;y^\star \\
\color{red}{g(x)} \text{ if } y \geq y^\star.
\end{cases}
\end{equation}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This means, that when we want to consider a new set of hyperparamters, we would like to draw it from the distribution that seems to produce lower values for our objective function, i.e. we want to draw from $\color{blue}{l(x)}$ [<a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">cp. Bergstra et al 2011</a>].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/blog/img/seminar/lstm_gru_1819/tpe.png" alt="title">
[<a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">cp. Koehrsen 2018</a>]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the figure above we see an (artificial) example from evaluating the loss function of a Random Forest, where $x$ is simply the number of estimators we wish to use. As expected, we see that, the higher the number of trees we include in our Random Forest, the lower the loss we can achieve. When we fix $y^\star=120$ we can separate two subsets from all the values of $x$ that we tried. When plotting their separate Kernel-Density estimates the red density corresponds to the values of $x$ that led to a high loss, whereas the blue distribution is formed by the values of $x$ which led to a low loss. Given that knowledge, when one is to choose a value of $x$ for the next evaluation common sense, and the Tree-structured Parzen Estimator approach encourage us to consider a value that is similar to the blue dots. Therefore we sample from $\color{blue}{l(x)}$, since these values seem to have worked in the previous iterations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Mathematical-Derivation">Mathematical Derivation<a class="anchor-link" href="#Mathematical-Derivation">&#182;</a></h4><p>As stated, the general idea is remarkably simple. However, the mathematical derivation is a little more convoluted. Formally, the Tree-structured Parzen Estimator's goal is to maximize the Expected Information gain $EI_{y^\star}$ for some fixed $y^\star$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
EI_{y^\star}(x) = \int\limits_{-\infty}^{y^\star} (y^\star-y)p(y|x)dy.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The expected information gain is a function of the hyperparameters, where we try to integrate out all interesting, values for y, i.e. those which are smaller than $y^\star$. We can also use Bayes' formula to find a different expression for $p(y|x)$</p>
$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But before we continue, it will become useful to note some things about the unconditional density of $p(x)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
p(x) &amp;= \int p(x, y) dy \\
     &amp;= \int p(x|y)p(y) dy \\
     &amp;= \int\limits_{\color{blue}{-\infty}}^{\color{blue}{y^\star}}\color{blue}{p(x|y)}p(y)dy +
        \int\limits^{\color{red}{\infty}}_{\color{red}{y^\star}}\color{red}{p(x|y)}p(y)dy \\
     &amp;= \color{blue}{l(x)} \int\limits_{-\infty}^{y^\star}p(y)dy +
        \color{red}{g(x)} \int\limits^{\infty}_{y^\star}p(y)dy \\
     &amp;= l(x) \underbrace{P(Y&lt;y^\star)}_{:=\gamma} + g(x)    \underbrace{P(Y \geq y^\star)}_{:=1-\gamma} \\
     &amp;= \gamma l(x) + (1-\gamma)g(x)
\end{align}<p>[<a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">cp. Bergstra et al 2011</a>]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What does this all mean? First, we can get the unconditional density of $x$ if we integrate out the influence of $y$ from the joint density $p(x, y)$. Second, we can replace the joint density with an expression we get from the definition of the conditional density of</p>
$$p(x|y) = \frac{p(x, y)}{p(y)} \Leftrightarrow p(x, y) = p(x|y)\ p(y).$$<p>Third, now that we have an expression that contains $p(x|y)$ we can make use of our definition from the beginning, i.e. we can split the integral into the '$\color{blue}{good}$' and '$\color{red}{bad}$' parts of the real line. And since $\color{blue}{l(x)}$ and $\color{red}{g(x)}$ do not depend on $y$, we can pull them out of the integral in the fourth line. This leaves us with integrals over the unconditional density of $y$ which is just how probabilities are defined. Summing up, this means we can describe the unconditional distribution of the hyperparameters as a weighted sum, where the weights are the probabilities of lying below or above the threshold $y^\star$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Coming back to our problem: we want to further investigate the information gain</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align}
EI_{y^\star}(x) &amp;= \int\limits_{-\infty}^{y^\star} (y^\star-y)p(y|x)dy \\
  &amp;= \int\limits_{\color{blue}{-\infty}}^{\color{blue}{y^\star}}(y^\star-y) \frac{\color{blue}{p(x|y)}p(y)}{p(x)}dy
  = \frac{\color{blue}{l(x)}}{p(x)} \int\limits_{-\infty}^{y^\star}(y^\star-y) p(y)dy \\
  &amp;= \frac{l(x)}{p(x)} \left[ y^\star \underbrace{\int\limits_{-\infty}^{y^\star} p(y)dy}_{P(Y &lt; y^\star) = \gamma} - \int\limits_{-\infty}^{y^\star} y \cdot p(y)dy \right]
  = \frac{l(x)\gamma y^\star - l(x)\int_{-\infty}^{y^\star} y \cdot p(y) dy}{l(x) \gamma + (1-\gamma)g(x)}
  \propto \color{orange}{\left[\gamma + (1-\gamma) \frac{g(x)}{l(x)}\right]^{-1}}
\end{align}<p>[<a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">cp. Bergstra et al 2011</a>]</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We start by applying Bayes rule to the conditional density of y $p(y|x)$ inside the interval and we note that this leaves us with an expression which is similar to the '$\color{blue}{good}$' part of the conditional density of $x$ given $y$. We pull all terms that do not depend on $y$ out of the integral and multiply out the parenthesis inside it. After multiplying we note that we can replace the integral over the unconditional density of $y$ with the probability that $P(Y&lt;y^\star):= \gamma$ and we note that we can replace $p(x)$ by the expression we derived previously. Unfortunately, we cannot further simplify the remaining integral, even if it looks like an expected value (in it's limits it is missing one half of the real line). Thus we can only factor out $l(x)$ in the numerator and denominator of the second last step such that it - almost - cancels out. Nonetheless, we can state that the numerator will just be a constant which means that the Expected Information Gain is proportional to the orange expression. This means that, as we outlined in the intuitive explanation, the Expected Information Gain grows if we are more likely to sample from the '$\color{blue}{good}$' density $\color{blue}{l(x)}$ than the '$\color{red}{bad}$' density $\color{red}{g(x)}$ [<a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">cp. Koehrsen 2018</a>].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Backpropagation">Backpropagation<a class="anchor-link" href="#Backpropagation">&#182;</a></h2><p>In this section we derive the backwards pass for both LSTM and GRU using high-school level calculus only.</p>
<h3 id="LSTM">LSTM<a class="anchor-link" href="#LSTM">&#182;</a></h3><p>$
% Forget-Gate
\newcommand{\F}{\color{orange}{f_t}}
\newcommand{\Ffull}[1][]{\color{orange}{\sigma^{#1} \left( W_f + U_f \cdot h_{t-1} \right)}}
\newcommand{\f}[1]{\color{orange}{f^t_{#1}}}
\newcommand{\ffull}[2][]{\color{orange}{\sigma^{#1} \left(w^f_{#2} x_t + u^f_{#2 1} h^{t-1}_1 + u^f_{#2 2}h^{t-1}_2 \right)}}
\newcommand{\Wf}{\color{orange}{W_f}}
\newcommand{\Uf}{\color{orange}{U_f}}
\newcommand{\wf}[1]{\color{orange}{w_{#1}^f}}
\newcommand{\uf}[2]{\color{orange}{u_{#1 #2}^f}}
% Input-Gate
\newcommand{\I}{\color{red}{i_t}}
\newcommand{\Ifull}[1][]{\color{red}{\sigma^{#1} \left( W_i + U_i \cdot h_{t-1} \right)}}
\newcommand{\i}[1]{\color{red}{i^t_{#1}}}
\newcommand{\ifull}[2][]{\color{red}{\sigma^{#1} \left( w^i_{#2} x_t + u^i_{#2 1} h^{t-1}_1 + u^i_{#2 2} h^{t-1}_2 \right)}}
\newcommand{\Wi}{\color{red}{W_i}}
\newcommand{\Ui}{\color{red}{U_i}}
\newcommand{\wi}[1]{\color{red}{w_{#1}^i}}
\newcommand{\ui}[2]{\color{red}{u_{#1 #2}^i}}
% Output-Gate
\newcommand{\O}{\color{blue}{o_t}}
\newcommand{\Ofull}[1][]{\color{blue}{\sigma^{#1} \left( W_o + U_o \cdot h_{t-1} \right)}}
\newcommand{\o}[1]{\color{blue}{o^t_{#1}}}
\newcommand{\ofull}[2][]{\color{blue}{\sigma^{#1} \left( w^o_{#2} x_t + u^o_{#2 1} h^{t-1}_1 + u^o_{#2 2} h^{t-1}_2 \right)}}
\newcommand{\Wo}{\color{blue}{W_o}}
\newcommand{\Uo}{\color{blue}{U_o}}
\newcommand{\wo}[1]{\color{blue}{w_{#1}^o}}
\newcommand{\uo}[2]{\color{blue}{u_{#1 #2}^o}}
% Cell (c without tilde - or however the fuck it's supposed to be called)
\newcommand{\C}[1][]{\color{lime}{c_{t #1}}}
\newcommand{\Cfull}{\F * \C[-1] + \I * \Ctil}
\newcommand{\c}[2][]{\color{lime}{c^{t #1}_{#2}}}
% Candidate (c with tilde - again: whatever, it's fine.)
\newcommand{\Ctil}{\color{green}{\tilde{c}_t}}
\newcommand{\Ctilfull}[1][]{\color{green}{g^{#1} \left( W_c x_t + U_c \cdot h_{t-1} \right)}}
\newcommand{\ctil}[2][]{\color{green}{g^{#1} \left( w^c_{#2} x_t + u^c_{#2 1} h^{t-1}_1 + u^c_{#2 2} h^{t-1}_2 \right)}}
\newcommand{\ctils}[1]{\color{green}{\tilde{c}_{#1}}}
\newcommand{\Wc}{\color{green}{W_c}}
\newcommand{\Uc}{\color{green}{U_c}}
\newcommand{\wc}[1]{\color{green}{w_{#1}^c}}
\newcommand{\uc}[2]{\color{green}{u_{#1 #2}^c}}
% Miscellaneous
\newcommand{\yhat}{\hat{y}_t}
\newcommand{\error}{\left(y - \yhat \right)}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dxt}[1]{\color{#1}{x_t}}
\DeclareMathOperator{\diag}{diag}
\newcommand{\myH}[2][-1]{\color{#2}{h_{t #1}}}
\newcommand{\myh}[2][red]{\color{#1}{h^{t-1}_{#2}}}
$
We first start by taking the neatly separated equations $(1)$ to $(6)$ and plug them all in, in order to get a more granular understanding of how our predictions are formed.</p>
\begin{equation}
\begin{split}
\yhat &amp;= w^T \cdot \myH[]{red}
      = w^T \cdot \left[ \O * g(\C) \right] \\
      &amp;= w^T \left[ \Ofull * g(\Cfull) \right] \\
      &amp;= w^T \cdot \Bigg[ \begin{pmatrix} \ofull{1} \\ \ofull{2} \end{pmatrix} \\
      &amp;\quad\quad* g
                         \begin{pmatrix}
                             \ffull{1} \c[-1]{1} + \ifull{1} + \ctil{1} \\
                             \ffull{2} \c[-1]{2} + \ifull{2} + \ctil{2}
                         \end{pmatrix} \Bigg]
\end{split}
\end{equation}<p>which, if written in its single equation form, gives</p>
\begin{alignat}{2}
\yhat &amp;= w_1 \ofull{1} g\left(\ffull{1} \c[-1]{1} + \\ \ifull{1} \ctil{1}\right)\\
      &amp;+ w_2 \ofull{2} g\left(\ffull{2} \c[-1]{2} + \\ \ifull{2} \ctil{2}\right) \tag{7}
\end{alignat}<p>Now this is arguably neither neat, nor pretty. However, since we are looking at a very low dimensional example it is a good opportunity to look into the backpropagation algorithm using high-school math only. And we can do that in a straightforward manner looking at equation $(7)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section we are going to take the partial derivative of $(7)$ with respect to each weight and using these results we will piece the gradients together. Moreover, once we have done that we will try to find a set of matrix operations, that will result in the gradients we previously puzzled together.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weights-of-the-output-gate">Weights of the output gate<a class="anchor-link" href="#Weights-of-the-output-gate">&#182;</a></h4><p>Let's start with the weights of the output gate. If we take the partial derivatives with respect to $\wo{1}$ and $\wo{2}$ respectively we obtain:</p>
\begin{equation}
\deriv{\yhat}{\wo{1}} = w_1 g(\c{1}) \ofull[\prime]{1} \ctil{1} \dxt{blue}
\end{equation}\begin{equation}
\deriv{\yhat}{\wo{2}} = w_2 g(\c{2}) \ofull[\prime]{2} \ctil{2} \dxt{blue}
\end{equation}<p>Which we now only need to combine in order to get the gradient of $\yhat$ with respect to $\Wo$.</p>
\begin{align}
\deriv{\yhat}{\Wo} &amp;= \begin{bmatrix} \deriv{\yhat}{\wo{1}} \\ \deriv{\yhat}{\wo{2}} \end{bmatrix} \\
                   &amp;= w * g(\C) * \Ofull[\prime] * \Ctil\ \dxt{blue} \\
                   &amp;= \diag(w) \cdot \diag(g(\C)) \cdot \diag(\Ofull[\prime]) \cdot \Ctil \ \dxt{blue}
\end{align}<p>The last line is just a way of describing the same formula, without the use of element-wise multiplication, relying just on scalar and dot products. However, while this satisfies more conventional algebraic notational standards it is computationally more efficient to implement the gradients using Hadamard products.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similarly, we obtain the partial derivatives for each element of $\Uo$.</p>
\begin{equation}
\deriv{\yhat}{\uo{1}{1}} = w_1 g(\c{1}) \ofull[\prime]{1} \ctil{1} \myh[blue]{1}
\end{equation}\begin{equation}
\deriv{\yhat}{\uo{1}{2}} = w_1 g(\c{1}) \ofull[\prime]{1} \ctil{1} \myh[blue]{2}
\end{equation}\begin{equation}
\deriv{\yhat}{\uo{2}{1}} = w_2 g(\c{2}) \ofull[\prime]{2} \ctil{2} \myh[blue]{1}
\end{equation}\begin{equation}
\deriv{\yhat}{\uo{2}{2}} = w_2 g(\c{2}) \ofull[\prime]{2} \ctil{2} \myh[blue]{2}
\end{equation}<p>And by combining them into the jacobian of $\yhat$ with respect to $\Uo$ we get</p>
\begin{align}
\deriv{\yhat}{\Uo} &amp;= \begin{bmatrix}
                         \deriv{\yhat}{\uo{1}{1}} &amp; \deriv{\yhat}{\uo{1}{2}} \\
                         \deriv{\yhat}{\uo{2}{1}} &amp; \deriv{\yhat}{\uo{2}{2}}
                     \end{bmatrix} \\
                   &amp;= \begin{bmatrix}
                         w_1 g(\c{1}) \ofull[\prime]{1} \ctils{1} \myh[blue]{1} &amp;
                         w_1 g(\c{1}) \ofull[\prime]{1} \ctils{1} \myh[blue]{2} \\
                         w_2 g(\c{2}) \ofull[\prime]{2} \ctils{2} \myh[blue]{1} &amp;
                         w_2 g(\c{2}) \ofull[\prime]{2} \ctils{2} \myh[blue]{2} \\
                     \end{bmatrix}  \\
                  &amp;= \begin{bmatrix} w &amp; w \end{bmatrix} *
                     \begin{bmatrix} g(\C) &amp; g(\C) \end{bmatrix} *
                     \begin{bmatrix} \Ofull[\prime] &amp; \Ofull[\prime] \end{bmatrix} *
                     \begin{bmatrix} \Ctil &amp; \Ctil \end{bmatrix} *
                     \begin{bmatrix} \myH{blue}^T \\ \myH{blue}^T \end{bmatrix}
                     \\
                  &amp;= \diag(w) \cdot \diag(g(\C)) \cdot \diag(\Ofull[\prime]) \cdot \diag(\Ctil) \cdot \mathbb{1} \mathbb{1}^T \cdot \diag(\myH{blue}).
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weights-of-the-forget-gate">Weights of the forget gate<a class="anchor-link" href="#Weights-of-the-forget-gate">&#182;</a></h4><p>By the same token we obtain the partial derivatives of $\yhat$ with respect to the elements of $\Wf$</p>
\begin{equation}
\deriv{\yhat}{\wf{1}} = w_1 \o{1} g^\prime(\c{1}) \ffull[\prime]{1} \c[-1]{1} \dxt{orange}
\end{equation}\begin{equation}
\deriv{\yhat}{\wf{2}} = w_2 \o{2} g^\prime(\c{2}) \ffull[\prime]{2} \c[-1]{2} \dxt{orange}
\end{equation}<p>and combine them to retrieve the gradient</p>
\begin{align}
\deriv{\yhat}{\Wf} &amp;= w * \O * g^\prime(\C) * \Ffull[\prime] * \C[-1] \ \dxt{orange} \\
                   &amp;= \diag(w) \cdot \diag(\O) \cdot \diag\left(g^\prime(\C)\right) \cdot \diag(\Ffull[\prime]) \cdot \C[-1] \ \dxt{orange}.
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the partial derivatives of $\yhat$ with respect to the elements of $\Uf$ we obtain:</p>
\begin{equation}
\deriv{\yhat}{\uf{1}{1}} = w_1 g^\prime(\c{1}) \ffull[\prime]{1} \c[-1]{1} \myh[orange]{1}
\end{equation}\begin{equation}
\deriv{\yhat}{\uf{1}{2}} = w_1 g^\prime(\c{1}) \ffull[\prime]{1} \c[-1]{1} \myh[orange]{2}
\end{equation}\begin{equation}
\deriv{\yhat}{\uf{2}{1}} = w_2 g^\prime(\c{2}) \ffull[\prime]{2} \c[-1]{2} \myh[orange]{1}
\end{equation}\begin{equation}
\deriv{\yhat}{\uf{2}{2}} = w_2 g^\prime(\c{2}) \ffull[\prime]{2} \c[-1]{2} \myh[orange]{2}.
\end{equation}<p>Which we can combine in order to acquire the jacobian of $\yhat$ with respect to $\Uf$.</p>
\begin{align}
\deriv{\yhat}{\Uf} &amp;= \begin{bmatrix}
                         \deriv{\yhat}{\uf{1}{1}} &amp; \deriv{\yhat}{\uf{1}{2}} \\
                         \deriv{\yhat}{\uf{2}{1}} &amp; \deriv{\yhat}{\uf{2}{2}}
                     \end{bmatrix}
                   = \begin{bmatrix}
                          w_1 g^\prime(\c{1}) \ffull[\prime]{1} \c[-1]{1} \myh[orange]{1} &amp;
                          w_1 g^\prime(\c{1}) \ffull[\prime]{1} \c[-1]{1} \myh[orange]{2} \\
                          w_2 g^\prime(\c{2}) \ffull[\prime]{2} \c[-1]{2} \myh[orange]{1} &amp;
                          w_2 g^\prime(\c{2}) \ffull[\prime]{2} \c[-1]{2} \myh[orange]{2}
                     \end{bmatrix}
                  \\
                  &amp;= \begin{bmatrix} w &amp; w \end{bmatrix} *
                     \begin{bmatrix} \O &amp; \O \end{bmatrix} *
                     g^\prime( \begin{bmatrix} \C &amp; \C \end{bmatrix}) *
                     \begin{bmatrix} \Ffull[\prime] &amp; \Ffull[\prime] \end{bmatrix} *
                     \begin{bmatrix} \C[-1] &amp; \C[-1] \end{bmatrix} *
                     \begin{bmatrix} \myH{orange}^T \\ \myH{orange}^T \end{bmatrix} \\
                  &amp;= \diag(w) \cdot \diag(\O) \cdot \diag\left(g^\prime(\C)\right) \cdot \diag(\Ffull[\prime]) \cdot \diag(\C[-1]) \cdot \mathbb{1} \mathbb{1}^T \cdot \diag(\myH{orange})
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weights-of-the-input-gate">Weights of the input gate<a class="anchor-link" href="#Weights-of-the-input-gate">&#182;</a></h4><p>Looking at the weights of the input gate we can proceed in the familiar fashion. We procure the partial derivatives of $\yhat$ with respect to $\wi{1}$ and $\wi{2}$</p>
\begin{equation}
\deriv{\yhat}{\wi{1}} = w_1 \o{1} g^\prime(\c{1}) \ctil{1} \ifull[\prime]{1} \dxt{red},
\end{equation}\begin{equation}
\deriv{\yhat}{\wi{2}} = w_2 \o{2} g^\prime(\c{2}) \ctil{2} \ifull[\prime]{2} \dxt{red}
\end{equation}<p>and proceed by collecting them in order to form the gradient of $\yhat$ with respect to $\Wi$</p>
\begin{align}
\deriv{\yhat}{\Ui} &amp;= w * \O * g^\prime(\C) * \Ctil * \Ifull[\prime] \dxt{red} \\
                   &amp;= \diag(w) \cdot \diag(\O) \cdot \diag(g^\prime(\C)) \cdot \diag(\Ctil) \cdot \Ifull[\prime] \dxt{red}.
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Equivalently we proceed with the partial derivatives of $\yhat$ with respect to the elements of $\Ui$, which are</p>
\begin{equation}
\deriv{\yhat}{\ui{1}{1}} = w_1 \o{1} g^\prime(\c{1}) \ctil{1} \ifull[\prime]{1} \myh[red]{1},
\end{equation}\begin{equation}
\deriv{\yhat}{\ui{1}{2}} = w_1 \o{1} g^\prime(\c{1}) \ctil{1} \ifull[\prime]{1} \myh[red]{2},
\end{equation}\begin{equation}
\deriv{\yhat}{\ui{2}{1}} = w_2 \o{2} g^\prime(\c{2}) \ctil{2} \ifull[\prime]{2} \myh[red]{1},
\end{equation}<p>and
\begin{equation}
\deriv{\yhat}{\ui{2}{2}} = w_2 \o{2} g^\prime(\c{2}) \ctil{2} \ifull[\prime]{2} \myh[red]{2}.
\end{equation}</p>
<p>By collecting them into a matrix we obtain the jacobian of $\yhat$ with respect to $\Ui$</p>
\begin{align}
\deriv{\yhat}{\Ui} &amp;= \begin{bmatrix}
                          \deriv{\yhat}{\ui{1}{1}} &amp; \deriv{\yhat}{\ui{1}{2}} \\
                          \deriv{\yhat}{\ui{2}{1}} &amp; \deriv{\yhat}{\ui{2}{2}}
                      \end{bmatrix} \\
                   &amp;= \begin{bmatrix}
                          w_1 \o{1} g^\prime(\c{1}) \ctils{1} \ifull[\prime]{1} \myh[red]{1} &amp;
                          w_1 \o{1} g^\prime(\c{1}) \ctils{1} \ifull[\prime]{1} \myh[red]{2} \\
                          w_2 \o{2} g^\prime(\c{2}) \ctils{2} \ifull[\prime]{2} \myh[red]{1} &amp;
                          w_2 \o{2} g^\prime(\c{2}) \ctils{2} \ifull[\prime]{2} \myh[red]{2} \\
                      \end{bmatrix} \\
                   &amp;= \begin{bmatrix} w &amp; w \end{bmatrix} *
                      \begin{bmatrix} \O &amp; \O \end{bmatrix} *
                      g^\prime\left(\begin{bmatrix} \C &amp; \C \end{bmatrix}\right) *
                      \begin{bmatrix} \Ctil &amp; \Ctil \end{bmatrix} *
                      \begin{bmatrix} \Ifull[\prime] &amp; \Ifull[\prime] \end{bmatrix} *
                      \begin{bmatrix} \myH{red}^T \\ \myH{red}^T \end{bmatrix} \\
                   &amp;= \diag(w) \cdot \diag(\O) \cdot \diag\left(g^\prime(\C)\right) \cdot \diag(\Ctil) \cdot \diag(\Ifull[\prime]) \cdot \mathbb{1} \mathbb{1}^T \cdot \diag(\myH{red})
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weights-of-the-candidate-update">Weights of the candidate update<a class="anchor-link" href="#Weights-of-the-candidate-update">&#182;</a></h3><p>Again we begin by taking partial derivatives of $\yhat$ with respect to $\Wc$'s elements, such that we get</p>
\begin{align}
\deriv{\yhat}{\wc{1}} = w_1 \o{1} g^\prime(\c{1}) \ifull{1} \ctil[\prime]{1} \dxt{green}
\end{align}<p>and</p>
\begin{align}
\deriv{\yhat}{\wc{2}} = w_2 \o{2} g^\prime(\c{2}) \ifull{2} \ctil[\prime]{2} \dxt{green}.
\end{align}<p>Arranging the partial derivatives, we obtain the gradient of $\yhat$ with respect to $\Wc$</p>
\begin{align}
\deriv{\yhat}{\Wc} = w * \O * g^\prime(\C) * \I * \Ctilfull[\prime]\ \dxt{green},
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similarly, we start by obtaining the partial derivatives of $\yhat$ with respect to each of $\Wc$'s elements.</p>
\begin{equation}
\deriv{\yhat}{\uc{1}{1}} = w_1 \o{1} g^\prime(\c{1}) \i{1} \ctil[\prime]{1} \myh[green]{1},
\end{equation}\begin{equation}
\deriv{\yhat}{\uc{1}{2}} = w_1 \o{1} g^\prime(\c{1}) \i{1} \ctil[\prime]{1} \myh[green]{2},
\end{equation}\begin{equation}
\deriv{\yhat}{\uc{2}{1}} = w_2 \o{2} g^\prime(\c{2}) \i{2} \ctil[\prime]{2} \myh[green]{1}
\end{equation}<p>and
\begin{equation}
\deriv{\yhat}{\uc{2}{2}} = w_2 \o{2} g^\prime(\c{2}) \i{2} \ctil[\prime]{2} \myh[green]{2}.
\end{equation}</p>
<p>And by organizing these partial derivatives, we can form the jacobian of $\yhat$ with respect to $\Uc$</p>
\begin{align}
\deriv{\yhat}{\Uc} &amp;= \begin{bmatrix}
                          \deriv{\yhat}{\uc{1}{1}} &amp; \deriv{\yhat}{\uc{1}{2}} \\
                          \deriv{\yhat}{\uc{2}{1}} &amp; \deriv{\yhat}{\uc{2}{2}}
                      \end{bmatrix} =
                      \begin{bmatrix}
                          w_1 \o{1} g^\prime(\c{1}) \i{1} \ctil[\prime]{1} \myh[green]{1} &amp;
                          w_1 \o{1} g^\prime(\c{1}) \i{1} \ctil[\prime]{1} \myh[green]{2} \\
                          w_2 \o{2} g^\prime(\c{2}) \i{2} \ctil[\prime]{2} \myh[green]{1} &amp;
                          w_2 \o{2} g^\prime(\c{2}) \i{2} \ctil[\prime]{2} \myh[green]{2}
                      \end{bmatrix} \\
                   &amp;= \begin{bmatrix} w &amp; w \end{bmatrix} *
                      \begin{bmatrix} \O &amp; \O \end{bmatrix} *
                      g^\prime(\begin{bmatrix} \C &amp; \C \end{bmatrix}) *
                      \begin{bmatrix} \I &amp; \I \end{bmatrix} *
                      \begin{bmatrix} \Ctilfull[\prime] &amp; \Ctilfull[\prime] \end{bmatrix} *
                      \begin{bmatrix} \myH{green}^T \\ \myH{green}^T \end{bmatrix} \\
                   &amp;= \diag(w) \cdot
                      \diag(\O) \cdot
                      \diag\left(g^\prime(\C)\right) \cdot
                      \diag(\I) \cdot
                      \diag(\Ctilfull[\prime]) \cdot
                      \mathbb{1} \mathbb{1}^T \cdot
                      \diag(\myH{green})
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="GRU">GRU<a class="anchor-link" href="#GRU">&#182;</a></h3><p>$
% Vector shortcuts
\newcommand{\Htil}{\color{green}{\tilde{h}_t}}
\newcommand{\H}{\color{green}{h_{t-1}}}
\newcommand{\R}{\color{blue}{r_t}}
\newcommand{\Z}{\color{red}{z_t}}
% Gates Matrix Notation
\newcommand{\Gfull}[1][]{\color{green}{g^{#1}\left( W_h x_t + U_h \cdot ( \R * \H)\right)}}
\newcommand{\Zfull}[1][]{\color{red}{\sigma^{#1} \left( W_z x_t + U_z \cdot h_{t-1} \right)}}
\newcommand{\Rfull}[1][]{\color{blue}{\sigma^{#1} \left( W_r x_t + U_r \cdot h_{t-1} \right)}}
% Vector Elements
\newcommand{\htil}[1]{\color{green}{\tilde{h}^t_{#1}}}
\newcommand{\h}[1]{\color{green}{h_{#1}^{t-1}}}
\newcommand{\z}[1]{\color{red}{z_{#1}^{t}}}
\newcommand{\r}[1]{\color{blue}{r_{#1}^{t}}}
% Explicit Vector Elements
\newcommand{\gfull}[2][]{\color{green}{g^{#1} \left( w^h_{#2} x_t + u^h_{#2 1} \r{1} \h{1} + u^h_{#2 2} \r{2} \h{2} \right)}}
\newcommand{\zfull}[2][]{\color{red}{\sigma^{#1} \left(w^z_{#2} x_t + u^z_{#2 1}h_1^{t-1}+u^z_{#2 2} h_2^{t-1}\right)}}
\newcommand{\rfull}[2][]{\color{blue}{\sigma^{#1} \left(w^r_{#2} x_t + u^r_{#2 1}h_1^{t-1}+u^r_{#2 2} h_2^{t-1}\right)}}
\newcommand{\gfullfull}[2][]{\color{green}{g^{#1} \left( w^h_{#2} x_t + u^h_{#2 1} \rfull{1} \h{1} + u^h_{#2 2} \rfull{2} \h{2} \right)}}
% Weight Matrices
\newcommand{\Wh}{\color{green}{W_h}}
\newcommand{\Wz}{\color{red}{W_z}}
\newcommand{\Wr}{\color{blue}{W_r}}
\newcommand{\Uh}{\color{green}{U_h}}
\newcommand{\Uz}{\color{red}{U_z}}
\newcommand{\Ur}{\color{blue}{U_r}}
% Weight Matrix Elements
\newcommand{\wh}[1]{\color{green}{w^h_{#1}}}
\newcommand{\wz}[1]{\color{red}{w^z_{#1}}}
\newcommand{\wr}[1]{\color{blue}{w^r_{#1}}}
\newcommand{\uh}[2]{\color{green}{u^h_{#1 #2}}}
\newcommand{\uz}[2]{\color{red}{u^z_{#1 #2}}}
\newcommand{\ur}[2]{\color{blue}{u^r_{#1 #2}}}
% Miscellaneous
\newcommand{\dxt}[1]{\color{#1}{x_t}}
\newcommand{\yhat}{\hat{y}_t}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\diag}{diag}
$
Consider again, our low-dimensional numerical example with only one case with features $x = \begin{pmatrix}x_1, &amp; x_2, &amp; x_3 \end{pmatrix}^T$ and label $y$. Additionally let's again assume we only have two neurons in our network. This means for the weight matrices</p>
\begin{alignat}{2}
\color{green}{W_h} &amp;= \begin{bmatrix} \wh{1} \\ \wh{2} \end{bmatrix}  \quad
\color{green}{U_h} &amp;&amp;= \begin{bmatrix} \uh{1}{1} &amp; \uh{1}{2} \\ \uh{2}{1} &amp; \uh{2}{2}\end{bmatrix}\\
\color{red}{W_z} &amp;= \begin{bmatrix} \wz{1} \\ \wz{2} \end{bmatrix}  \quad
\color{red}{U_z} &amp;&amp;= \begin{bmatrix} \uz{1}{1} &amp; \uz{1}{2} \\ \uz{2}{1} &amp; \uz{2}{2} \end{bmatrix}\\
\color{blue}{W_r} &amp;= \begin{bmatrix} \wr{1} \\ \wr{2} \end{bmatrix} \quad
\color{blue}{U_r} &amp;&amp;= \begin{bmatrix} \ur{1}{1} &amp; \ur{1}{2} \\ \ur{2}{1} &amp; \ur{2}{2} \end{bmatrix}
\end{alignat}<p>We define the potential candidate update as
\begin{align}
\Htil &amp;= \Gfull \\ &amp;= \begin{pmatrix} \gfull{1} \\ \gfull{2} \end{pmatrix} \\ &amp;= \begin{pmatrix} \gfullfull{1} \\ \gfullfull{2} \end{pmatrix},
\end{align}
the gates as
\begin{align}
\Z = \Zfull &amp;= \begin{pmatrix} \z{1} \\ \z{2} \end{pmatrix} = \begin{pmatrix} \zfull{1} \\ \zfull{2} \end{pmatrix}  \quad\text{and}\\
\R = \Rfull &amp;= \begin{pmatrix} \r{1} \\ \r{2} \end{pmatrix} = \begin{pmatrix} \rfull{1} \\ \rfull{2} \end{pmatrix},
\end{align}
where $g$ is the $tanh$-function and $\sigma$ the sigmoid function.</p>
<p>Finally, we define the update of the hidden state as
\begin{align}
h_t &amp;= \color{red}{1-\Z} * h_{t-1} + \Z * \Htil \\
    &amp;=
\begin{pmatrix}
\color{red}{(1-\z{1})} h^{t-1}_1 + \z{1} \gfull{1} \\
\color{red}{(1-\z{2})} h^{t-1}_2 + \z{2} \gfull{2}
\end{pmatrix} \\
    &amp;=
\begin{pmatrix}
\color{red}{(1-\zfull{1})} h^{t-1}_1 + \zfull{1} \gfullfull{1} \\
\color{red}{(1-\zfull{2})} h^{t-1}_2 + \zfull{2} \gfullfull{2}
\end{pmatrix}
\end{align} [<a href="https://arxiv.org/pdf/1701.05923.pdf">cp Rey, Salem, 2017</a>].</p>
<p>For the actual prediction at a step $t$, we connect every output of our hidden state in a dense layer. Which means that we are taking a weighted sum of all two of them.</p>
\begin{align}
\hat{y}_t &amp;= W^T h_t = w_1 h^t_1 + w_2 h^t_2 \\ &amp;= w_1 \color{red}{(1-\zfull{1})} h^{t-1}_1 \\&amp;\quad + \zfull{1} \gfullfull{1} \\ &amp;+ w_2
\color{red}{(1-\zfull{2})} h^{t-1}_2 \\&amp;\quad+ \zfull{2} \gfullfull{2},
\end{align}<p>Where $W = \begin{pmatrix} w_1 \\ w_2 \end{pmatrix}$ is the matrix that contains the weights for the outer layer. In our case this is just a $2\times 1$ vector.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weights-for-the-candidate-$\tilde{h}_t$">Weights for the candidate $\tilde{h}_t$<a class="anchor-link" href="#Weights-for-the-candidate-$\tilde{h}_t$">&#182;</a></h4><p>Let's start with taking partial derivatives of $\hat{y}_t$ with respect to the elemens of $\Wh$:</p>
\begin{equation}
\frac{\partial \hat{y}_t}{\partial \wh{1}} = w_1 \z{1} \gfull[\prime]{1} \color{green}{x_t}
\end{equation}<p>Which is just an application of the chain-rule for derivatives. Similarly we get</p>
\begin{equation}
\frac{\partial \hat{y}_t}{\partial \wh{2}} = w_2 \z{2} \gfull[\prime]{2} \color{green}{x_t}.
\end{equation}<p>We can obtain the gradient by combining the two partial derivatives in the following manner</p>
\begin{align}
\frac{\partial \hat{y}_t}{\partial \Wh} &amp;=
    \begin{bmatrix}
        \frac{\partial \hat{y}_t}{\partial \wh{1}} \\ \frac{\partial \hat{y}_t}{\partial \wh{2}}
    \end{bmatrix}                       =
    \begin{bmatrix}
        w_1 \z{1} \gfull{1} \color{green}{x_t} \\
        w_2 \z{2} \gfull{2} \color{green}{x_t}
    \end{bmatrix} \\
                                        &amp;= W * \Z * \Gfull[\prime] \color{green}{x_t}.
\end{align}<p>Additionally, we can make use of the element-wise product in order to obtain a formulation that has a straightforward <code>numpy</code> implementation. We get the same result if we deploy the logic of the backpropagation algorithm.</p>
\begin{align}
\frac{\partial {loss}_t}{\partial W_h} &amp;= \frac{\partial {loss}_t}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial \tilde{h}_t}\frac{\partial \tilde{h}_t}{\partial \Wh} \\
                                   &amp;= -(y-\hat{y}_t) W * \Z * \Gfull[\prime] \color{green}{x_t}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We continue with the partial derivatives of $\hat{y}_t$ with respect to the elements of $\Uh$</p>
\begin{align}
\frac{\partial \hat{y}_t}{\partial \uh{1}{1}} &amp;= w_1 \z{1} \gfull[\prime]{1} \r{1} \h{1} \\
\frac{\partial \hat{y}_t}{\partial \uh{1}{2}} &amp;= w_1 \z{1} \gfull[\prime]{1} \r{2} \h{2} \\
\frac{\partial \hat{y}_t}{\partial \uh{2}{1}} &amp;= w_2 \z{2} \gfull[\prime]{2} \r{1} \h{1} \\
\frac{\partial \hat{y}_t}{\partial \uh{2}{2}} &amp;= w_2 \z{2} \gfull[\prime]{2} \r{2} \h{2}.
\end{align}<p>These we can combine to form the jacobian of of $\yhat$ with respect to $\Uh$</p>
\begin{align}
\deriv{\yhat}{\Uh} &amp;=
\begin{bmatrix}
    \deriv{\yhat}{\uh{1}{1}} &amp; \deriv{\yhat}{\uh{1}{2}} \\
    \deriv{\yhat}{\uh{2}{1}} &amp; \deriv{\yhat}{\uh{2}{2}}
\end{bmatrix}      =
\begin{bmatrix}
w_1 \z{1} \gfull[\prime]{1} \r{1} \h{1} &amp;
w_1 \z{1} \gfull[\prime]{1} \r{2} \h{2} \\
w_2 \z{2} \gfull[\prime]{2} \r{1} \h{1} &amp;
w_2 \z{2} \gfull[\prime]{2} \r{2} \h{2}.
\end{bmatrix} \\
                  &amp;=
\begin{bmatrix} w_1 &amp; w_1 \\ w_2 &amp; w_2 \end{bmatrix} *
\begin{bmatrix} \z{1} &amp; \z{1} \\ \z{2} &amp; \z{2} \end{bmatrix} *
\begin{bmatrix} \gfull[\prime]{1} &amp; \gfull[\prime]{1} \\ \gfull[\prime]{2} &amp; \gfull[\prime]{2} \end{bmatrix} *
\begin{bmatrix} \r{1} &amp; \r{2} \\ \r{1} &amp; \r{2} \end{bmatrix} *
\begin{bmatrix} \h{1} &amp; \h{2} \\ \h{1} &amp; \h{2} \end{bmatrix} \\
                  &amp;=
\begin{bmatrix} w &amp; w \end{bmatrix} * \begin{bmatrix} \Z &amp; \Z \end{bmatrix} * \begin{bmatrix} \Gfull[\prime] &amp; \Gfull[\prime] \end{bmatrix} * \begin{bmatrix} \R^T \\ \R^T \end{bmatrix} * \begin{bmatrix} \H^T \\ \H^T \end{bmatrix} \\
&amp;= \diag(w) \cdot \diag(\Z) \cdot \diag(\Gfull[\prime]) \cdot \mathbb{1} \mathbb{1}^T \cdot \diag(\R) \cdot \diag(\H)
\end{align}<p>Again, following the logic of the backpropagation algorithm, we obtain the same result</p>
\begin{align}
\frac{\partial {loss}_t}{\partial \Uh} &amp;= \frac{\partial {loss}_t}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial \tilde{h}_t}\frac{\partial \tilde{h}_t}{\partial \Uh} \\
                                    &amp;= -(y-\hat{y}_t)\ \diag \left[ w * \Z * \Gfull[\prime]\right] \cdot  \begin{bmatrix} (\R * \H)^T \\ (\R * \H)^T \end{bmatrix}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weights-for-the-update-gate-$z_t$">Weights for the update gate $z_t$<a class="anchor-link" href="#Weights-for-the-update-gate-$z_t$">&#182;</a></h4><p>Using the same logic, we calculate the partial derivatives of $\yhat$ with respect to the elements of $\Wz$, which gives</p>
\begin{equation}
\deriv{\yhat}{\wz{1}} = w_1 \left(\color{red}{-} \zfull[\prime]{1}\right) \h{1} + \htil{1} \zfull[\prime]{1} \dxt{red},
\end{equation}\begin{equation}
\deriv{\yhat}{\wz{2}} = w_2 \left(\color{red}{-} \zfull[\prime]{2}\right) \h{1} + \htil{1} \zfull[\prime]{2} \dxt{red},
\end{equation}<p>and when combined leaves us the gradient of $\yhat$ with respect to $\Wz$ as</p>
\begin{align}
\deriv{\yhat}{\Wz} &amp;= \begin{bmatrix}
\deriv{\yhat}{\wz{1}} \\ \deriv{\yhat}{\wz{2}}
\end{bmatrix} =
\begin{bmatrix}
    w_1 \left(\color{red}{-} \zfull[\prime]{1}\right) \h{1} + \htil{1} \zfull[\prime]{1} \dxt{red} \\
    w_2 \left(\color{red}{-} \zfull[\prime]{2}\right) \h{1} + \htil{1} \zfull[\prime]{2} \dxt{red}
\end{bmatrix} \\
&amp;= w * \begin{bmatrix} -\H + \Htil \end{bmatrix} * \Zfull[\prime] \dxt{red}
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By now this should be getting old. We take partial derivatives of $\yhat$ with respect to each element of $\Uz$. This leaves us with</p>
\begin{align}
\deriv{\yhat}{\uz{1}{1}} &amp;= w_1 \left[\color{red}{(- \zfull[\prime]{1} \h{1} )} \h{1} + \htil{1} \zfull[\prime]{1} \h{1} \right], \\
\deriv{\yhat}{\uz{1}{2}} &amp;= w_1 \left[\color{red}{(- \zfull[\prime]{1} \h{2} )} \h{1} + \htil{1} \zfull[\prime]{1} \h{2} \right], \\
\deriv{\yhat}{\uz{2}{1}} &amp;= w_2 \left[\color{red}{(- \zfull[\prime]{2} \h{1} )} \h{2} + \htil{2} \zfull[\prime]{2} \h{1} \right] \text{and} \\
\deriv{\yhat}{\uz{2}{2}} &amp;= w_2 \left[\color{red}{(- \zfull[\prime]{2} \h{2} )} \h{2} + \htil{2} \zfull[\prime]{2} \h{2} \right].
\end{align}<p>We can combine these partial derivatives in order to form the jacobian of $\yhat$ with respect to $\Uz$ as</p>
\begin{align}
\deriv{\yhat}{\Uz} &amp;=
\begin{bmatrix}
    \deriv{\yhat}{\uz{1}{1}} &amp; \deriv{\yhat}{\uz{1}{2}} \\
    \deriv{\yhat}{\uz{2}{1}} &amp; \deriv{\yhat}{\uz{2}{2}}
\end{bmatrix} \\ &amp;=
\begin{bmatrix}
     w_1 \left[\color{red}{(- \zfull[\prime]{1} \h{1} )} \h{1} + \htil{1} \zfull[\prime]{1} \h{1} \right] &amp;
     w_1 \left[\color{red}{(- \zfull[\prime]{1} \h{2} )} \h{1} + \htil{1} \zfull[\prime]{1} \h{2} \right] \\
     w_2 \left[\color{red}{(- \zfull[\prime]{2} \h{1} )} \h{2} + \htil{2} \zfull[\prime]{2} \h{1} \right] &amp;
     w_2 \left[\color{red}{(- \zfull[\prime]{2} \h{2} )} \h{2} + \htil{2} \zfull[\prime]{2} \h{2} \right]
\end{bmatrix} \\ &amp;=
\begin{bmatrix} w &amp; w \end{bmatrix} *
\begin{bmatrix} \Htil - \H &amp; \Htil - \H \end{bmatrix} *
\begin{bmatrix} \Zfull[\prime] &amp; \Zfull[\prime] \end{bmatrix} *
\begin{bmatrix} \H^T \\ \H^T \end{bmatrix} \\
&amp;= \diag(w) \cdot \diag(\Htil - \H) \cdot \Zfull[\prime] \cdot \H^T.
\end{align}<p>And by the same token, we need to use the previous derivations in order to find the derivative of the loss function with respect to $\Uz$, where now one can easily see that:</p>
\begin{align}
\frac{\partial {loss}_t}{\partial \Uz} &amp;= \frac{\partial loss}{\partial \hat{y}_t}\frac{\partial \hat{y}_t}{\partial h_t}\frac{\partial h_t}{\partial z_t}\frac{\partial z_t}{\partial \Uz} \\
                                   &amp;= -(y-\hat{y}_t)\ \diag \left[ w * [-\H + \Htil] * \Zfull[\prime]\right] \cdot \begin{bmatrix} \H^T \\ \H^T \end{bmatrix}.
\end{align}<p>Having done that, we can implement it in the now too familiar way.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Weights-for-the-reset-gate-$r_t$">Weights for the reset gate $r_t$<a class="anchor-link" href="#Weights-for-the-reset-gate-$r_t$">&#182;</a></h4><p>Similarly, we can proceed with obtaining the gradients of $\yhat$ with respect to the elements of the reset gate.</p>
\begin{align}
\deriv{\yhat}{\wr{1}} &amp;= w_1 \z{1} \gfull[\prime]{1} \uh{1}{1} \h{1} \rfull[\prime]{1} \dxt{blue} \\
&amp;\quad + w_2 \z{2} \gfull[\prime]{2} \uh{2}{1} \h{1} \rfull[\prime]{1} \dxt{blue}
\end{align}<p>and</p>
\begin{align}
\deriv{\yhat}{\wr{2}} &amp;= w_1 \z{1} \gfull[\prime]{1} \uh{1}{2} \h{2} \rfull[\prime]{2} \dxt{blue} \\
&amp;\quad + w_2 \z{2} \gfull[\prime]{2} \uh{2}{2} \h{2} \rfull[\prime]{2} \dxt{blue}.
\end{align}<p>By collecting the partial derivatives into a vector we form the gradient of $\yhat$ with respect to $\Wr$</p>
\begin{align}
\deriv{\yhat}{\Wr} &amp;= \begin{bmatrix}\deriv{\yhat}{\wr{1}} \\ \deriv{\yhat}{\wr{2}} \end{bmatrix} \\
&amp;= w^T \cdot \left(
\begin{bmatrix} \Z &amp; \Z \end{bmatrix} *
\begin{bmatrix} \Gfull[\prime] &amp; \Gfull[\prime] \end{bmatrix} *
\Uh *
\begin{bmatrix} \H^T \\ \H^T \end{bmatrix} *
\begin{bmatrix} \Rfull[\prime]^T \\ \Rfull[\prime]^T \end{bmatrix}
\right) \dxt{blue} \\
&amp;= w^T \cdot \diag(\Z) \cdot \diag(\Gfull[\prime]) \cdot \Uh \cdot \diag(\H)\cdot \diag(\Rfull[\prime]) \dxt{blue}.
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we compute the partial derivatives of $\yhat$ with respect to $\Ur$</p>
\begin{align}
\deriv{\yhat}{\ur{1}{1}} &amp;= w_1 \z{1} \gfull[\prime]{1} \uh{1}{1} \h{1} \rfull[\prime]{1} \h{1} \\
                         &amp; \quad + w_2 \z{2} \gfull[\prime]{2} \uh{2}{1} \h{1} \rfull[\prime]{1} \h{1} \\
\deriv{\yhat}{\ur{1}{2}} &amp;= w_1 \z{1} \gfull[\prime]{1} \uh{1}{1} \h{1} \rfull[\prime]{1} \h{2} \\
                         &amp; \quad + w_2 \z{2} \gfull[\prime]{2} \uh{2}{1} \h{1} \rfull[\prime]{1} \h{2} \\
\deriv{\yhat}{\ur{2}{1}} &amp;= w_1 \z{1} \gfull[\prime]{1} \uh{1}{2} \h{2} \rfull[\prime]{2} \h{1} \\
                         &amp; \quad + w_2 \z{2} \gfull[\prime]{2} \uh{2}{2} \h{2} \rfull[\prime]{2} \h{1} \\
\deriv{\yhat}{\ur{2}{2}} &amp;= w_1 \z{1} \gfull[\prime]{1} \uh{1}{2} \h{2} \rfull[\prime]{2} \h{2} \\
                         &amp; \quad + w_2 \z{2} \gfull[\prime]{2} \uh{2}{2} \h{2} \rfull[\prime]{2} \h{2}.
\end{align}<p>And by combining all partial derivatives, we obtain the jacobian of $\yhat$ with respect to $\Ur$</p>
\begin{align}
\deriv{\yhat}{\Ur} &amp;=
\begin{bmatrix}
    \deriv{\yhat}{\ur{1}{1}} &amp; \deriv{\yhat}{\ur{1}{2}} \\
    \deriv{\yhat}{\ur{2}{1}} &amp; \deriv{\yhat}{\ur{2}{2}}
\end{bmatrix} \\ &amp;=
\begin{bmatrix}(w * \Z * \Gfull[\prime])^T \\ (w * \Z * \Gfull[\prime])^T \end{bmatrix} \cdot \Uh^T  *
\begin{bmatrix} \H \cdot \H^T \end{bmatrix} *
\begin{bmatrix} \Rfull[\prime] &amp; \Rfull[\prime] \end{bmatrix} \\ &amp;=
\diag(w) \cdot \diag(\Z) \cdot \diag(\Gfull[\prime]) \cdot \Uh^T \cdot \diag(\H) \cdot \diag(\Rfull[\prime]) \cdot \mathbb{1}\mathbb{1}^T \cdot \diag(\H).
\end{align}<p>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References">&#182;</a></h1><ol>
<li>Jeremy Howard (2018): Introduction to practical Deep Learning for Coders – Lesson 6: RNNs. <a href="http://course18.fast.ai/lessons/lesson6.html">http://course18.fast.ai/lessons/lesson6.html</a> (accessed: 06.02.2019)</li>
<li>Thomas Fischer &amp; Christopher Krauss, (2018): Deep learning with long short-term memory networks for financial market predictions. European Journal of Operation Research Vol 270 Issue 2</li>
<li>Jason Brownlee, 2016: Stateful and Stateless LSTM for Time Series Forecasting with Python. <a href="https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/">https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/</a> (accessed: 06.02.2019)</li>
<li>Yumi (2018) : Stateful LSTM model training in Keras. <a href="https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html">https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html</a> (accessed: 06.02.2019)</li>
<li>Philippe Remey (2016): Stateful LSTM in Keras. <a href="http://philipperemy.github.io/keras-stateful-lstm/">http://philipperemy.github.io/keras-stateful-lstm/</a> (accessed: 06.02.2019)</li>
<li>Cathy O'Neil, (2016): Weapons of Math Destruction. How Big Data increases inequality and threatens democracy. </li>
<li>Arun Mallya (2017): <a href="http://arunmallya.github.io/">http://arunmallya.github.io/</a> (accessed: 05.02.2019).</li>
<li>Rahul Dey, Fathi M. Salem (2017): Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks. <a href="https://arxiv.org/pdf/1701.05923.pdf">https://arxiv.org/pdf/1701.05923.pdf</a> (accessed: 05.02.2019).</li>
<li>Will Koehrsen (2018): A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning. <a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f</a> (accessed: 05.02.2019).</li>
<li>James Bergstra, Remi Bardenet, Yoshua Bengio, Balázs Kégl (2011): Algorithms for Hyper-Parameter Optimization. <a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf</a> (accessed: 05.02.2019).</li>
<li>Harini Sureshi (2016):  Sequence Modeling with Neural Networks. <a href="http://introtodeeplearning.com/2017/Sequence%20Modeling.pdf">http://introtodeeplearning.com/2017/Sequence%20Modeling.pdf</a> (accessed: 05.12.2018).</li>
<li>Christopher Olah (2015): Understanding LSTM Networks. <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a> (accessed: 29.11.2018)</li>
<li>Michael Nguyen (2018): Illustrated Guide to LSTM’s and GRU’s: A step by step explanation. <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-tep-by-step-explanation-44e9eb85bf21">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-tep-by-step-explanation-44e9eb85bf21</a> (accessed: 05.02.2019)</li>
<li>Andrew Tch (2017): The mostly complete chart of Neural Networks, explained. <a href="https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-fb6f2367464">https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-fb6f2367464</a> (accessed: 29.11.2018)</li>
<li>Thomas Fischer &amp; Christpher Krauss (2017): Deep learning with long short-term memory networks for financial market predictions. ttps://www.econstor.eu/bitstream/10419/157808/1/886576210.pdf (accessed: 05.02.2019)</li>
<li>Hochreiter and Schmidhuber (1997): Long Short-Term Memory <a href="https://www.bioinf.jku.at/publications/older/2604.pdf">https://www.bioinf.jku.at/publications/older/2604.pdf</a> (accessed: 05.02.2019)</li>
<li>Alexander Stec, Diego Klabjan (2018): Forecasting Crime with Deep Learning. <a href="https://arxiv.org/pdf/1806.01486.pdf">https://arxiv.org/pdf/1806.01486.pdf</a> (accessed: 05.02.2019)</li>
<li>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio (2014): Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. <a href="https://arxiv.org/pdf/1406.1078.pdf">https://arxiv.org/pdf/1406.1078.pdf</a> (accessed: 05.02.2019)</li>
<li>Rui Fu, Zuo Zhang, and Li Li (2016) :Using LSTM and GRU Neural Network Methods for Traffic Flow Prediction. ttps://www.researchgate.net/profile/Li_Li240/publication/312402649_Using_LSTM_and_GRU_neural_network_methods_for_traffic_flow_prediction/links/5c20d38d299bf12be3971696/Using-LSTM-and-GRU-neural-network-methods-for-traffic-flow-prediction.pdf (accessed: 05.02.2019)</li>
</ol>

</div>
</div>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {}, "version_minor": 0, "version_major": 2}
</script>

                        </div>
                        
                        

                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        








<div class="panel panel-default sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title">Categories</h3>
    </div>

    <div class="panel-body">
        <ul class="nav nav-pills nav-stacked">
            
            <li><a href="https://humboldt-wi.github.io/blog/categories/course-projects">course-projects (31)</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/categories/instruction">instruction (2)</a>
            </li>
            
        </ul>
    </div>
</div>











<div class="panel sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title">Tags</h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/a/b-testing"><i class="fa fa-tags"></i> a/b-testing</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/attention"><i class="fa fa-tags"></i> attention</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/awd-lstm"><i class="fa fa-tags"></i> awd-lstm</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/bayesian-deep-learning"><i class="fa fa-tags"></i> bayesian-deep-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/bayesian-topic-modelling"><i class="fa fa-tags"></i> bayesian-topic-modelling</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/black-box"><i class="fa fa-tags"></i> black-box</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/blockchain"><i class="fa fa-tags"></i> blockchain</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/causal-inference"><i class="fa fa-tags"></i> causal-inference</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/class17/18"><i class="fa fa-tags"></i> class17/18</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/class18/19"><i class="fa fa-tags"></i> class18/19</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/class19"><i class="fa fa-tags"></i> class19</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/classification"><i class="fa fa-tags"></i> classification</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/coarsened-exact-matching"><i class="fa fa-tags"></i> coarsened-exact-matching</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/conversion"><i class="fa fa-tags"></i> conversion</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/convolutional-neural-networks"><i class="fa fa-tags"></i> convolutional-neural-networks</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/data-simulation"><i class="fa fa-tags"></i> data-simulation</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/deep-learning"><i class="fa fa-tags"></i> deep-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/distant-transfer-learning"><i class="fa fa-tags"></i> distant-transfer-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/dml"><i class="fa fa-tags"></i> dml</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/doc2vec"><i class="fa fa-tags"></i> doc2vec</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/document-embeddings"><i class="fa fa-tags"></i> document-embeddings</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/explanation"><i class="fa fa-tags"></i> explanation</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/fine-tuning"><i class="fa fa-tags"></i> fine-tuning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/genetic-matching"><i class="fa fa-tags"></i> genetic-matching</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/gru"><i class="fa fa-tags"></i> gru</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/hierarchical-network"><i class="fa fa-tags"></i> hierarchical-network</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/ice"><i class="fa fa-tags"></i> ice</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/image-analysis"><i class="fa fa-tags"></i> image-analysis</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/image-captioning"><i class="fa fa-tags"></i> image-captioning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/inference"><i class="fa fa-tags"></i> inference</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/keras-imdb-dataset"><i class="fa fa-tags"></i> keras-imdb-dataset</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/knn-algorithm"><i class="fa fa-tags"></i> knn-algorithm</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/language-modelling"><i class="fa fa-tags"></i> language-modelling</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/lda"><i class="fa fa-tags"></i> lda</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/lime"><i class="fa fa-tags"></i> lime</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/long-short-term-memory"><i class="fa fa-tags"></i> long-short-term-memory</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/lstm"><i class="fa fa-tags"></i> lstm</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/machine-learning"><i class="fa fa-tags"></i> machine-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/matching-methods"><i class="fa fa-tags"></i> matching-methods</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/matchit"><i class="fa fa-tags"></i> matchit</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/monte-carlo-dropout"><i class="fa fa-tags"></i> monte-carlo-dropout</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/movie-reviews"><i class="fa fa-tags"></i> movie-reviews</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/nearest-neighbor"><i class="fa fa-tags"></i> nearest-neighbor</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/neural-network"><i class="fa fa-tags"></i> neural-network</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/neural-networks"><i class="fa fa-tags"></i> neural-networks</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/nlp"><i class="fa fa-tags"></i> nlp</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/optimal-matching"><i class="fa fa-tags"></i> optimal-matching</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/pdp"><i class="fa fa-tags"></i> pdp</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/pretraining"><i class="fa fa-tags"></i> pretraining</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/propensity-score"><i class="fa fa-tags"></i> propensity-score</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/propensity-score-weighting"><i class="fa fa-tags"></i> propensity-score-weighting</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/recommendation"><i class="fa fa-tags"></i> recommendation</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/recommender-system"><i class="fa fa-tags"></i> recommender-system</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/recommender-systems"><i class="fa fa-tags"></i> recommender-systems</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/rnn"><i class="fa fa-tags"></i> rnn</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/rs"><i class="fa fa-tags"></i> rs</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/sentiment-analysis"><i class="fa fa-tags"></i> sentiment-analysis</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/sentiment-classification"><i class="fa fa-tags"></i> sentiment-classification</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/share-price-prediction"><i class="fa fa-tags"></i> share-price-prediction</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/simulation"><i class="fa fa-tags"></i> simulation</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/text-analysis"><i class="fa fa-tags"></i> text-analysis</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/text-mining"><i class="fa fa-tags"></i> text-mining</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/time-series"><i class="fa fa-tags"></i> time-series</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/time-series-forecasting"><i class="fa fa-tags"></i> time-series-forecasting</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/transfer-learning"><i class="fa fa-tags"></i> transfer-learning</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/treatment-effect"><i class="fa fa-tags"></i> treatment-effect</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/twitter"><i class="fa fa-tags"></i> twitter</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/ulmfit"><i class="fa fa-tags"></i> ulmfit</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/uplift"><i class="fa fa-tags"></i> uplift</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/uplift-modelling"><i class="fa fa-tags"></i> uplift-modelling</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/variational-inference"><i class="fa fa-tags"></i> variational-inference</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/wikitext-103"><i class="fa fa-tags"></i> wikitext-103</a>
            </li>
            
            <li><a href="https://humboldt-wi.github.io/blog/tags/word-embeddings"><i class="fa fa-tags"></i> word-embeddings</a>
            </li>
            
        </ul>
    </div>
</div>



















                        

                    </div>
                    

                    

                </div>
                

            </div>
            
        </div>
        

        <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<footer id="footer">
    <div class="container">

        

        <div class="col-md-4 col-sm-6">

             
            

            
                
                
                    
                        
                          
                            
                          
                        
                    
                    
                        
                    
                
                
                
                    
                        
                          
                            
                          
                        
                    
                    
                        
                    
                
                
                
                    
                        
                          
                            
                          
                        
                    
                    
                        
                    
                
                
            

            <hr class="hidden-md hidden-lg">
             

        </div>
        

        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-12">
            
            <p class="pull-left">Copyright (c) 2017, Chair of Information System at HU-Berlin; all rights reserved.</p>
            
            <p class="pull-right">
              Template by <a href="http://bootstrapious.com/free-templates">Bootstrapious</a>.
              

              Ported to Hugo by <a href="https://github.com/devcows/hugo-universal-theme">DevCows</a>
            </p>
        </div>
    </div>
</div>





    </div>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-112025566-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="//maps.googleapis.com/maps/api/js?v=3.exp"></script>

<script src="https://humboldt-wi.github.io/blog/js/hpneo.gmaps.js"></script>
<script src="https://humboldt-wi.github.io/blog/js/gmaps.init.js"></script>
<script src="https://humboldt-wi.github.io/blog/js/front.js"></script>


<script src="https://humboldt-wi.github.io/blog/js/owl.carousel.min.js"></script>


  </body>
</html>
